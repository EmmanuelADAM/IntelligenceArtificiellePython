{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/introHuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace : des modèles entraînés\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) est un site recueillant les modèles IA entraînés, les plus connus (GPT, Bart, Mistral...) comme ses plus modestes.\n",
    "Ce site mets aussi à disposition des dataset (comme le site kaggle).\n",
    "\n",
    "Un modèle comprend plusieurs éléments. Par exemple, pour le texte, il contiendra un vocabulaire, l'outil de \"tokenisation\", un réseau entrainé, et un outil de restitution du résultat.\n",
    "\n",
    "\n",
    "Les modèles peuvent être téléchargés en local (attention à la taille de certains), ou utilisés en ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transformers\n",
    "Les transformers sont utilisés pour convertir une entrée (texte par exemple) en entrée assimilable par un ensemble d'outils contenant un réseau de neurones.\n",
    "\n",
    "Il est nécessaire de le télécharger comme une librairie classique : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\anaconda\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\anaconda\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\anaconda\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\anaconda\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "#téléchargement, un peu long parfois, de la librairie transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation en mémoire\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## pipeline\n",
    "Un pipeline est une suite d'outils (tokenizer, analyseur, ...) visant un but précis.\n",
    "\n",
    "Par exemple, le pipeline suivant analyse le \"sentiment\" d'un texte : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52164e3c862740948ec20a7962a854c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emmanuel adam\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment-latest. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca300bb3ffb4149859a33d54a9ed1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68ae6556e254bce8894f34b512a22a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2874ce3138d04722a5ec98e92939b294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46484c6e290e49d0aec0059507bb9517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#chargement du \"detecteur de sentiments\" par défaut\n",
    "#la première utilisation prend un peu de temps de téléchargement\n",
    "detecteur_de_sentiments = pipeline(\"sentiment-analysis\")\n",
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classement par defaut = \n",
      "I like discover new thing in my courses at UPHF.\n",
      "\t classifier par defaut :  {'label': 'POSITIVE', 'score': 0.9983413219451904}\n",
      "\t classifier cardiff :  {'label': 'positive', 'score': 0.9546298980712891}\n",
      "But I hate writing my practical work reports !\n",
      "\t classifier par defaut :  {'label': 'NEGATIVE', 'score': 0.9973371624946594}\n",
      "\t classifier cardiff :  {'label': 'negative', 'score': 0.9157376289367676}\n",
      "I should better sleep at night rather than in the courses\n",
      "\t classifier par defaut :  {'label': 'NEGATIVE', 'score': 0.9995858073234558}\n",
      "\t classifier cardiff :  {'label': 'neutral', 'score': 0.5100991129875183}\n"
     ]
    }
   ],
   "source": [
    "phrases =     [\n",
    "        \"I like discover new thing in my courses at UPHF.\",\n",
    "        \"But I hate writing my practical work reports !\", \n",
    "        \"I should better sleep at night rather than in the courses\"\n",
    "    ]\n",
    "\n",
    "default_output = detecteur_de_sentiments(phrases)\n",
    "cardiff_output =pipe(phrases)\n",
    "print(\"classement par defaut = \")\n",
    "for i in range(len(phrases)):\n",
    "    print(phrases[i])\n",
    "    print(\"\\t classifier par defaut : \", default_output[i])\n",
    "    print(\"\\t classifier cardiff : \", cardiff_output[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Choix du modèle\n",
    "Sur huggingFace, il est assez simple de choisir son modèle : \n",
    "  - cliquez en haut au centre sur **Models**\n",
    "  - choisissez dans **Natural Language Processing** le traitement désiré (par exemple TextClassification)\n",
    "  - choisissez ensuite le modèle voulu, ou plus téléchargé, ou le plus \"liké\"\n",
    "  - cliquez sur **</> use in transformers**\n",
    "  - vous obtenez ainsi le code pour charger le pipeline (pour ce TP, on choisira \"Use a pipeline as a high-level helper\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Travail à faire\n",
    "\n",
    "Peu de code dans ce TP. \n",
    "\n",
    "  1 - **Classement** : Reprenez le dataset sur le classement de livres et tester 3 modèles sur les 100 premières données du dataset. Comparez les résultats.\n",
    "\n",
    "  2 - **Résumé automatique** : prenez le dataset [samsun](https://huggingface.co/datasets/samsum) et testez  3 modèles de résumé sur les 20 premières  lignes du dataset. Evaluez par vous même la qualité des résumés.\n",
    "  \n",
    "  3 - **Génération de texte**. Chargez trois modèles générateurs de texte et demandez la recette des panckakes et évaluez les réponses. \n",
    "    - commencez la recette : sentences = generator(\"To make pancakes, I need flour?\", max_length=30, num_return_sequences=5,  return_full_text=True)<br>\n",
    "    ceci demande la suite du texte, 5 fois, avec des réponses de 30 mots max \n",
    "  \n",
    "  4 - **Réponse à tout** : utilisez 3 modèles de question-answering et posez 3 questions sur le texte anglais de votre choix. Comparez.\n",
    "\n",
    "**A rendre**, les codes, les questions et réponses. Ainsi votre réflexion sur la confiance envers les modèles existants. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7715b250ca453ca6bd01f7e3e3f6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emmanuel adam\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b843d8ba8e0468aa03bef58458b682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8a7f4db05d4a8f8abd214708a79385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a259751efe49c998a1e8f8575edee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cd223a9b8d49c29108c8c53fe567ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#exemple d'utilisation de \"question-answering\"\n",
    "pipe = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from when comes artificial intelligence ?  -  {'score': 0.07508504390716553, 'start': 119, 'end': 123, 'answer': '1956'}\n",
      "what is the concept behind artificial intelligence ?  -  {'score': 0.7541843056678772, 'start': 533, 'end': 546, 'answer': 'automated art'}\n"
     ]
    }
   ],
   "source": [
    "context = \"The academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[20] Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[21] The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[22][23] The tradition of creative automatons has flourished throughout history, exemplified by Maillardet's automaton created in the early 1800s.[24]\"\n",
    "\n",
    "question = \"from when comes artificial intelligence ?\"\n",
    "output = pipe({\"context\": context, \"question\": question })\n",
    "print(question, \" - \", output)\n",
    "question = \"what is the concept behind artificial intelligence ?\"\n",
    "output = pipe({\"context\": context, \"question\": question })\n",
    "print(question, \" - \", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
