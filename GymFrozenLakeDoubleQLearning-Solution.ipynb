{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/GymFrozenLakeDoubleQLearning-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI](https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test de ML par Double Q-Learning pour atteindre l'objectif\n",
    "\n",
    "**Utilisation de l'environnement Gym**\n",
    " (voir la page d'introduction à [Gym](https://gym.openai.com))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Si besoin, importer gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L'environnement FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Utiliser l'environnement `FrozenLake8x8-v1` (un labyrinthe en mode texte)\n",
    "  - **ATTENTION**, avec d'ancienne version de gym (sous colab, ...), il faut utiliser la version 0 (`FrozenLake8x8-v0`)\n",
    "- 4 actions sont possibles (Left(0), Down(1), Right(2), Up(3))\n",
    "  - l'adjectif \"Frozen\" signifie qu'une *action n'est pas déterministe !*\n",
    "    - à partir d'une case \"gelée\", aller à droite peut .. mener à droite, ou pas\n",
    "    - => intérêt du Q-Learning adapté à ce type d'environnement probabiliste\n",
    "- Le labyrinthe est ainsi composé de zones glacées, de puits, et d'un objectif\n",
    "\n",
    "\n",
    "**N.B.** \n",
    "  - *Cet environnement fonctionne bien sous colab, jupyterlab.. quelques soucis de l'affichage de l'état courant (carré rouge) sous Pyzo....* \n",
    "  - Il est fortement conseillé de débuter avec un environnement déterministe pour évaluer la bonne marche de l'algo de Q-Learning que vous aurez développer.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Etude de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specification de l'environnement :  EnvSpec(FrozenLake-v1)\n",
      "espace d'actions :  Discrete(4)  => 4 actions \"discretes\" (non continues)\n",
      "espace d'etats :  Discrete(16)  => 16 etats distincts\n",
      "Environnement et etat initial (en rouge) : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "S = Start (pos 0), G = Goal (pos 15), H = Hole, F = Frozen place\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False) # tester FrozenLake8x8 pour l'environnement plus large\n",
    "print(\"specification de l'environnement : \", env.spec)\n",
    "print(\"espace d'actions : \", env.action_space , \" => 4 actions \\\"discretes\\\" (non continues)\") #ici 4 actions discrétisée\n",
    "print(\"espace d'etats : \", env.observation_space , \" => 16 etats distincts\") #ici 4x4 cellules possibles\n",
    "\n",
    "env.reset()\n",
    "print(\"Environnement et etat initial (en rouge) : \")\n",
    "env.render()\n",
    "print(\"S = Start (pos 0), G = Goal (pos 15), H = Hole, F = Frozen place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Test des actions\n",
    "\n",
    "Sous Gym, `step` permet d'effectuer une action. \n",
    "En retour la fonction retourne une observation sur l'etat d'arrivee, sa recompense, son type (final ou non), et des informations.\n",
    "Ici, dans FrozenLake, \n",
    "- observation = position où se trouve l'agent\n",
    "- reward = recompense\n",
    "- done = vrai si but atteint\n",
    "- info = probabilité de succès de l'action \n",
    "  - en mode déterministe, sol non glissant, la proba de réussite est de 100%\n",
    "  - en mode non déterministe, sol glissant, la proba de réussite est de 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "###### Test des actions\n",
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 7 ,gain: 0.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Cas non déterministe**\n",
    "\n",
    "L'environnement FrozenLake peut également être chargé en mode non déterministe : chaque état est une case gelée, et chaque action qui s'y deroule n'a qu'une chance sur trois de réussir ! Et 2 chances sur 3 de mener ailleurs !!\n",
    "\n",
    "Chargeons l'environnement dans ce mode et testons les actions à partir de l'état initial : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=True) \n",
    "\n",
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0.0 ,fini: True , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est clairement ici dans un environnement non déterministe (une même action à partir d'un même état ne mène pas toujours au même résultat); c'est le contexte de prédilection de l'algo de Q-Learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Premiere résolution en mode déterministe</font>\n",
    "Important, pour valider l'apprentissage de votre algorithme avant de passer en mode non-déterministe, il vaut mieux le tester sur un environnement où chaque action à 100% de réussite. Ci-dessous un exemple sur le mini labyrinthe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 6 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "pos° actuelle: 14 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "pos° actuelle: 15 ,gain: 1.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.reset()\n",
    "actions = [2,2,1,1,1,2]\n",
    "for a in actions:\n",
    "    observation, reward, done, info = env.step(a)\n",
    "    env.render()\n",
    "    print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## L'algorithme de Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1', is_slippery=False)\n",
    "\n",
    "actions = {0:'Gauche', 1:'Bas', 2:'Droite', 3:'Haut'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise en place des paramètres\n",
    "L'algo de **double** Q Learning simple repose sur ces équations :\n",
    "\n",
    "$a^* \\gets  argmax_{a} QA(s', a)$\n",
    "\n",
    "$QA(s,a) \\gets QA(s,a) + \\lambda \\times (r + \\gamma \\times QB(s', a^*))-  QA(s,a))$ \n",
    "\n",
    "et\n",
    "\n",
    "$b^* \\gets  argmax_{a} QB(s', a)$\n",
    "\n",
    "$QB(s,a) \\gets QB(s,a) + \\lambda \\times (r + \\gamma \\times QA(s', b^*))-  QB(s,a))$ \n",
    "\n",
    "avec\n",
    "  - $\\lambda$ : coef d'apprentissage\n",
    "  - $\\gamma$ : coef de réduction \n",
    "  - $r$ : récompense\n",
    "  \n",
    "Cette équation donne la qualité de l'action *a* à partir de l'état *s*.\n",
    "\n",
    "Initialement, les actions sont choisies aléatoirement et notées; puis au fil des tests les actions les plus valuées sont choisies. Pour cela, un tirage est effectuée, s'il est inférieur à un $\\epsilon$, le choix est aléatoire. Cet $\\epsilon$ décroit au fil des tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiser la Q-Table\n",
    "# autant de cases que l'environnement en possède, \n",
    "# contenant autant de valeurs que d'actions possibles\n",
    "# donc ici une matrice 64 x 4\n",
    "QA = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "QB = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "lambda_learn = .1\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Les outils de Python\n",
    "#### Récupérer la meilleure action\n",
    "`argmax(tab)` retourne l'indice de la plus grande valeur du tableau.\n",
    "\n",
    "`argmax(Q[2])` retourne donc le no de l'action la plus intéressante à partir de l'état 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tab1=np.array([[9,3,1], [2,6,9]], float)\n",
    "print(np.argmax(tab1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab2 = np.array([[0,5,9], [9,8,7]], float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construire un tableau reprenant les valeurs maxi de 2 tableaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fusion des valeurs max de la 1ere ligne de tab1 & tab2\n",
    "maxis = np.max([tab1[0, :], tab2[0, :]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion des valeurs maxi entre  [9. 3. 1.]  et  [0. 5. 9.]  =  [9. 5. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(\"fusion des valeurs maxi entre \", tab1[0, :], \" et \", tab2[0, :],\" = \", maxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indices des plus hautes valeurs d'un tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_max dans  [9. 5. 9.]  =  9.0\n",
      "indices où se trouve le max =  [0 2]\n"
     ]
    }
   ],
   "source": [
    "val_max = np.max(maxis)\n",
    "print(\"val_max dans \", maxis, \" = \", val_max)\n",
    "max_i = np.where(maxis == val_max)[0]\n",
    "print(\"indices où se trouve le max = \", max_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Choix d'une valeur dans un tableau\n",
    "np.random.choice(max_i, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### L'algorithme de Double Q-Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##algorithme de Q-Learning simple\n",
    "def q_learn(nb_steps=64):\n",
    "    \"\"\"\n",
    "    effectue un cycle d'apprentissage/recherche de solution' via le double Q-Learning\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nb_steps : nb de tests d'actions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_r : recompense totale\n",
    "    r : recompense du dernier etat rencontre\n",
    "    states_list : liste des etats traverses\n",
    "    actions_list : liste des actions effectuees\n",
    "\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    best_next_a = best_next_b = 0\n",
    "    best_ab = None\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done and step < nb_steps:\n",
    "        step += 1\n",
    "        # find the best actions values ammong the actions available at the current state, from QA & QB\n",
    "        best_ab = np.max([QA[s, :], QB[s, :]], axis=0)\n",
    "        # list of the actions with the maximum value in QA or QB\n",
    "        max_a = np.where(best_ab == np.max(best_ab))[0]\n",
    "        # choose one of the best action   \n",
    "        # initially, all the action has a 0 value; so initially the algo explores\n",
    "        # when there are sufficient values, the algo reinforces the actions\n",
    "        a = np.random.choice(max_a, 1)[0]\n",
    "        # play the action and get new state and reward from environment\n",
    "        sprime, r, done, _ = env.step(a)\n",
    "        # Q-Learning, play sometimes whith QA, or QB \n",
    "        if(rnd.randint(0,1)==0):            \n",
    "            best_next_a = np.argmax(QA[sprime, :])\n",
    "            QA[s, a] = QA[s, a] + lambda_learn*(r + gamma * QB[sprime, best_next_a] - QA[s, a])\n",
    "        else:\n",
    "            best_next_b = np.argmax(QB[sprime, :])\n",
    "            QB[s, a] = QB[s, a] + lambda_learn*(r + gamma * QA[sprime, best_next_b] - QB[s, a])\n",
    "\n",
    "        s = sprime\n",
    "        total_r = total_r + r\n",
    "        states_list.append(s)\n",
    "        actions_list.append(a)\n",
    "    return total_r, r, states_list, actions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_qlearn(nb_tests=2000, nb_steps=64):\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    solutions_list = []\n",
    "    epsilon = 1\n",
    "    for i in range(nb_tests):\n",
    "        # Reset environment and get first new observation\n",
    "        total_r, r, states_list, actions_list = q_learn(nb_steps)\n",
    "        # memorize if a solution has been found\n",
    "        if r == 1: solutions_list.append((i, total_r, states_list, actions_list))\n",
    "        \n",
    "    if(len(solutions_list) == 0): print(\"aucune solution trouvee !!\")\n",
    "\n",
    "    return solutions_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de du résultat\n",
    "Affichons maintenant la liste des actions via l'environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendu(solutions_list):\n",
    "    \"\"\" affiche la plus courte sequence d'actions permettant d'atteindre l'objectif q partir des solutions fournies\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions_list : liste des solutions trouvees\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    mini_sol = solutions_list[0]\n",
    "    for s in  solutions_list:\n",
    "        if len(s[2]) < len(mini_sol[2]): mini_sol = s\n",
    "    print(\"une solution en \", len(mini_sol[2]), \" etapes : \")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i in range(0, len(mini_sol[2])):\n",
    "        env.env.s = mini_sol[2][i]\n",
    "        print(\"action \", actions[mini_sol[3][i]])\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une solution en  14  etapes : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHF\u001b[41mF\u001b[0mFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFF\u001b[41mF\u001b[0mF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn(4000, 64)\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "#relancer le bloc si pas de solution trouvee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gain est intéressant. Si on doit parcrourir tout l'arbre de recherche, la complexité de l'arbre est borné par $4 \\times 4 \\times \\dots \\times 4 = 4^{63} = 85 070 591 730 234 615 865 843 651 857 942 052 864$ solutions à balayer (85 millards de millards de millards de millards de solutions)..\n",
    "\n",
    "Ici, $1000 \\times 25$ actions on été testées..\n",
    "\n",
    "Traçons une courbe pour évaluer la progression de l'apprentissage entre chaque test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_frequence_sol(solutions_list):\n",
    "    \"\"\"\n",
    "    dessine la frequence de solution trouvees\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions : liste des solutions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    xs = [x[0] for x in solutions_list]\n",
    "    ys = [y[1] for y in solutions_list]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xs, ys, '.')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.  , 0.03, 0.  ],\n",
       "       [0.  , 0.  , 0.04, 0.  ],\n",
       "       [0.  , 0.06, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.08, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.12, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.17, 0.  ],\n",
       "       [0.  , 0.  , 0.24, 0.  ],\n",
       "       [0.  , 0.  , 0.34, 0.  ],\n",
       "       [0.  , 0.49, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.7 , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(QA, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Test de résolution en mode non déterministe</font>\n",
    "Rechargeons l'environnement en mode \"glissant\".\n",
    "\n",
    "Il suffit de réinitialiser la table Q et de lancer l'algorithme...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une solution en  22  etapes : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Gauche\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Gauche\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFH\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Gauche\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', is_slippery=True)\n",
    "\n",
    "env.reset()\n",
    "QA = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "QB = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "lambda_learn = .1\n",
    "gamma = 0.9\n",
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn(10000, 50)\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus on relance les tests, plus on a de chance de trouver une solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.01, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.01, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.01],\n",
       "       [0.03, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.07, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.03, 0.  ],\n",
       "       [0.  , 0.  , 0.1 , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.26, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.53, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(QB, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
