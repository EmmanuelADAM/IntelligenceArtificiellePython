{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/GymFrozenLakeDoubleQLearning-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI](https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test de ML par Double Q-Learning pour atteindre l'objectif\n",
    "\n",
    "**Utilisation de l'environnement Gym**\n",
    " (voir la page d'introduction à [Gym](https://gym.openai.com))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Si besoin, importer gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L'environnement FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Utiliser l'environnement `FrozenLake8x8-v1` (un labyrinthe en mode texte)\n",
    "  - **ATTENTION**, avec d'ancienne version de gym (sous colab, ...), il faut utiliser la version 0 (`FrozenLake8x8-v0`)\n",
    "- 4 actions sont possibles (Left(0), Down(1), Right(2), Up(3))\n",
    "  - l'adjectif \"Frozen\" signifie qu'une *action n'est pas déterministe !*\n",
    "    - à partir d'une case \"gelée\", aller à droite peut .. mener à droite, ou pas\n",
    "    - => intérêt du Q-Learning adapté à ce type d'environnement probabiliste\n",
    "- Le labyrinthe est ainsi composé de zones glacées, de puits, et d'un objectif\n",
    "\n",
    "\n",
    "**N.B.** \n",
    "  - *Cet environnement fonctionne bien sous colab, jupyterlab.. quelques soucis de l'affichage de l'état courant (carré rouge) sous Pyzo....* \n",
    "  - Il est fortement conseillé de débuter avec un environnement déterministe pour évaluer la bonne marche de l'algo de Q-Learning que vous aurez développer.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Etude de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specification de l'environnement :  EnvSpec(FrozenLake-v1)\n",
      "espace d'actions :  Discrete(4)  => 4 actions \"discretes\" (non continues)\n",
      "espace d'etats :  Discrete(16)  => 16 etats distincts\n",
      "Environnement et etat initial (en rouge) : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "S = Start (pos 0), G = Goal (pos 15), H = Hole, F = Frozen place\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False) # tester FrozenLake8x8 pour l'environnement plus large\n",
    "print(\"specification de l'environnement : \", env.spec)\n",
    "print(\"espace d'actions : \", env.action_space , \" => 4 actions \\\"discretes\\\" (non continues)\") #ici 4 actions discrétisée\n",
    "print(\"espace d'etats : \", env.observation_space , \" => 16 etats distincts\") #ici 4x4 cellules possibles\n",
    "\n",
    "env.reset()\n",
    "print(\"Environnement et etat initial (en rouge) : \")\n",
    "env.render()\n",
    "print(\"S = Start (pos 0), G = Goal (pos 15), H = Hole, F = Frozen place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Test des actions\n",
    "\n",
    "Sous Gym, `step` permet d'effectuer une action. \n",
    "En retour la fonction retourne une observation sur l'etat d'arrivee, sa recompense, son type (final ou non), et des informations.\n",
    "Ici, dans FrozenLake, \n",
    "- observation = position où se trouve l'agent\n",
    "- reward = recompense\n",
    "- done = vrai si but atteint\n",
    "- info = probabilité de succès de l'action \n",
    "  - en mode déterministe, sol non glissant, la proba de réussite est de 100%\n",
    "  - en mode non déterministe, sol glissant, la proba de réussite est de 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "###### Test des actions\n",
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 7 ,gain: 0.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "##on se met sur la case 6 (plutôt que la case 0 par défaut)\n",
    "env.env.s = 6\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Cas non déterministe**\n",
    "\n",
    "L'environnement FrozenLake peut également être chargé en mode non déterministe : chaque état est une case gelée, et chaque action qui s'y deroule n'a qu'une chance sur trois de réussir ! Et 2 chances sur 3 de mener ailleurs !!\n",
    "\n",
    "Chargeons l'environnement dans ce mode et testons les actions à partir de l'état initial : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=True) \n",
    "\n",
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 7 ,gain: 0.0 ,fini: True , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0.0 ,fini: True , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 6\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est clairement ici dans un environnement non déterministe (une même action à partir d'un même état ne mène pas toujours au même résultat); c'est le contexte de prédilection de l'algo de Q-Learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Premiere résolution en mode déterministe</font>\n",
    "Important, pour valider l'apprentissage de votre algorithme avant de passer en mode non-déterministe, il vaut mieux le tester sur un environnement où chaque action à 100% de réussite. Ci-dessous un exemple sur le mini labyrinthe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 6 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "pos° actuelle: 14 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "pos° actuelle: 15 ,gain: 1.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.reset()\n",
    "actions = [2,2,1,1,1,2]\n",
    "for a in actions:\n",
    "    observation, reward, done, info = env.step(a)\n",
    "    env.render()\n",
    "    print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## L'algorithme de Double Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1', is_slippery=False)\n",
    "\n",
    "actions = {0:'Gauche', 1:'Bas', 2:'Droite', 3:'Haut'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise en place des paramètres\n",
    "L'algo de **double** Q Learning simple repose sur DEUX matrices nb_etats x nb_actions; la mise à jour des valeurs d'une action dans une grille utilise la valeur de la meilleure action suivante dans l'autre matrice.\n",
    "\n",
    "On réalise, plus ou moins alternativement :\n",
    "\n",
    "$a^* \\gets  argmax_{a} QA(s', a)$,\n",
    "\n",
    "$QA(s,a) \\gets QA(s,a) + \\lambda \\times (r + \\gamma \\times QB(s', a^*))-  QA(s,a))$ \n",
    "\n",
    "ou\n",
    "\n",
    "$b^* \\gets  argmax_{a} QB(s', a)$\n",
    "\n",
    "$QB(s,a) \\gets QB(s,a) + \\lambda \\times (r + \\gamma \\times QA(s', b^*))-  QB(s,a))$ \n",
    "\n",
    "avec\n",
    "  - $\\lambda$ : coef d'apprentissage\n",
    "  - $\\gamma$ : coef de réduction \n",
    "  - $r$ : récompense\n",
    "  \n",
    "Cette équation donne la qualité de l'action *a* à partir de l'état *s*.\n",
    "\n",
    "Initialement, à chaque état, une action est choisie aléatoirement (car toutes \"valent\" 0); puis au fil des tests les actions les plus valuées sont choisies. \n",
    "\n",
    "*Vous pouvez voir en fin de cette page des aides sur les fonctionnalités Python utiles pour la manipulation de tableaux.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiser la Q-Table\n",
    "# autant de cases que l'environnement en possède, \n",
    "# contenant autant de valeurs que d'actions possibles\n",
    "# donc ici une matrice 64 x 4\n",
    "QA = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "QB = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "lambda_learn = .1\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### L'algorithme de Double Q-Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##algorithme de Q-Learning simple\n",
    "def q_learn(nb_steps=64):\n",
    "    \"\"\"\n",
    "    effectue un cycle d'apprentissage/recherche de solution' via le double Q-Learning\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nb_steps : nb de tests d'actions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_r : recompense totale\n",
    "    r : recompense du dernier etat rencontre\n",
    "    states_list : liste des etats traverses\n",
    "    actions_list : liste des actions effectuees\n",
    "\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    best_next_a = best_next_b = 0\n",
    "    best_ab = None\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done and step < nb_steps:\n",
    "        step += 1\n",
    "        # find the best actions values ammong the actions available at the current state, from QA & QB\n",
    "        best_ab = np.max([QA[s, :], QB[s, :]], axis=0)\n",
    "        # list of the actions with the maximum value in QA or QB\n",
    "        max_a = np.where(best_ab == np.max(best_ab))[0]\n",
    "        # choose one of the best action   \n",
    "        # initially, all the action has a 0 value; so initially the algo explores\n",
    "        # when there are sufficient values, the algo reinforces the actions\n",
    "        a = np.random.choice(max_a, 1)[0]\n",
    "        # play the action and get new state and reward from environment\n",
    "        sprime, r, done, _ = env.step(a)\n",
    "        # Q-Learning, play sometimes whith QA, or QB \n",
    "        if(rnd.randint(0,1)==0):            \n",
    "            best_next_a = np.argmax(QA[sprime, :])\n",
    "            QA[s, a] = QA[s, a] + lambda_learn*(r + gamma * QB[sprime, best_next_a] - QA[s, a])\n",
    "        else:\n",
    "            best_next_b = np.argmax(QB[sprime, :])\n",
    "            QB[s, a] = QB[s, a] + lambda_learn*(r + gamma * QA[sprime, best_next_b] - QB[s, a])\n",
    "\n",
    "        s = sprime\n",
    "        total_r = total_r + r\n",
    "        states_list.append(s)\n",
    "        actions_list.append(a)\n",
    "    return total_r, r, states_list, actions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_qlearn(nb_tests=2000, nb_steps=64):\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    solutions_list = []\n",
    "    epsilon = 1\n",
    "    for i in range(nb_tests):\n",
    "        # Reset environment and get first new observation\n",
    "        total_r, r, states_list, actions_list = q_learn(nb_steps)\n",
    "        # memorize if a solution has been found\n",
    "        if r == 1: solutions_list.append((i, total_r, states_list, actions_list))\n",
    "        \n",
    "    if(len(solutions_list) == 0): print(\"aucune solution trouvee !!\")\n",
    "\n",
    "    return solutions_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de du résultat\n",
    "Affichons maintenant la liste des actions via l'environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendu(solutions_list):\n",
    "    \"\"\" affiche la plus courte sequence d'actions permettant d'atteindre l'objectif q partir des solutions fournies\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions_list : liste des solutions trouvees\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    mini_sol = solutions_list[0]\n",
    "    for s in  solutions_list:\n",
    "        if len(s[2]) < len(mini_sol[2]): mini_sol = s\n",
    "    print(\"une solution en \", len(mini_sol[2]), \" etapes : \")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i in range(0, len(mini_sol[2])):\n",
    "        env.env.s = mini_sol[2][i]\n",
    "        print(\"action \", actions[mini_sol[3][i]])\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une solution en  14  etapes : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHF\u001b[41mF\u001b[0mFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFF\u001b[41mF\u001b[0mF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn(3000, 64)\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "#relancer le bloc si pas de solution trouvee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gain est intéressant. Si on doit parcrourir tout l'arbre de recherche, la complexité de l'arbre est borné par $4 \\times 4 \\times \\dots \\times 4 = 4^{63} = 85 070 591 730 234 615 865 843 651 857 942 052 864$ solutions à balayer (85 millards de millards de millards de millards de solutions)..\n",
    "\n",
    "Ici, au plus $3000 \\times 64 = 192 000$ actions on été testées..\n",
    "\n",
    "Traçons une courbe pour évaluer la progression de l'apprentissage entre chaque test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_frequence_sol(solutions_list):\n",
    "    \"\"\"\n",
    "    dessine la frequence de solution trouvees\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions : liste des solutions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    xs = [x[0] for x in solutions_list]\n",
    "    ys = [y[1] for y in solutions_list]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xs, ys, '.')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.  , 0.03, 0.  ],\n",
       "       [0.  , 0.  , 0.04, 0.  ],\n",
       "       [0.  , 0.  , 0.06, 0.  ],\n",
       "       [0.  , 0.08, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.12, 0.  ],\n",
       "       [0.  , 0.  , 0.17, 0.  ],\n",
       "       [0.  , 0.24, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.34, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.49, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.7 , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##on regarde ce qu'il y a dans une des matrices : \n",
    "np.round(QA, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Test de résolution en mode non déterministe</font>\n",
    "Rechargeons l'environnement en mode \"glissant\".\n",
    "\n",
    "Il suffit de réinitialiser la table Q et de lancer l'algorithme...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une solution en  16  etapes : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHF\u001b[41mF\u001b[0mFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFF\u001b[41mF\u001b[0mF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFH\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFF\u001b[41mF\u001b[0mF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', is_slippery=True)\n",
    "\n",
    "env.reset()\n",
    "QA = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "QB = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "lambda_learn = .1\n",
    "gamma = 0.9\n",
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn(10000, 50)\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus on relance les tests, plus on a de chance de trouver une solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.01, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.  , 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.01],\n",
       "       [0.  , 0.  , 0.  , 0.01],\n",
       "       [0.  , 0.02, 0.  , 0.  ],\n",
       "       [0.02, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.03, 0.  , 0.  ],\n",
       "       [0.04, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.01, 0.  ],\n",
       "       [0.  , 0.03, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.09, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.14, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.53, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##on regarde ce qu'il y a dans une des matrices : \n",
    "np.round(QB, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <font color=\"red\">CORRECTION (travail à faire)</font>\n",
    "\n",
    "Un des problèmes de l'algo présenté ici est l'abandon assez rapide de l'exploration car dès qu'une action a une utilité plus grande que les autres, elle sera toujours choisie.\n",
    "\n",
    "Modifier légèrement le code précédent pour ajouter cette notion d'aléatoire...\n",
    "Cela peut être rapide (2 lignes à modifier) ou légèrement plus long..\n",
    "\n",
    "Quel est l'impact dans la résolution dans un environnement déterministe ? non déterministe ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### AIDE : Les outils de Python\n",
    "#### Récupérer la meilleure action\n",
    "`argmax(tab)` retourne l'indice de la plus grande valeur du tableau.\n",
    "\n",
    "`argmax(Q[2])` retourne donc le no de l'action la plus intéressante à partir de l'état 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tab1=np.array([[9,3,1], [2,6,9]], float)\n",
    "print(np.argmax(tab1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab2 = np.array([[0,5,9], [9,8,7]], float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construire un tableau reprenant les valeurs maxi de 2 tableaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fusion des valeurs max de la 1ere ligne de tab1 & tab2\n",
    "maxis = np.max([tab1[0, :], tab2[0, :]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion des valeurs maxi entre  [9. 3. 1.]  et  [0. 5. 9.]  =  [9. 5. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(\"fusion des valeurs maxi entre \", tab1[0, :], \" et \", tab2[0, :],\" = \", maxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indices des plus hautes valeurs d'un tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_max dans  [9. 5. 9.]  =  9.0\n",
      "indices où se trouve le max =  [0 2]\n"
     ]
    }
   ],
   "source": [
    "val_max = np.max(maxis)\n",
    "print(\"val_max dans \", maxis, \" = \", val_max)\n",
    "max_i = np.where(maxis == val_max)[0]\n",
    "print(\"indices où se trouve le max = \", max_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Choix d'une valeur dans un tableau\n",
    "np.random.choice(max_i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34249034 0.91912667 0.14298058]\n",
      " [0.20880007 0.84881274 0.17893182]]\n"
     ]
    }
   ],
   "source": [
    "#### générer un tableau avec des valeurs aléatoires\n",
    "mat = np.random.random((2,3))\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0034249  0.00919127 0.00142981]\n",
      " [0.002088   0.00848813 0.00178932]]\n"
     ]
    }
   ],
   "source": [
    "mat = mat / 100.\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
