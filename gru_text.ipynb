{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de réseaux récurent prédisant du texte\n",
    "## Utilisation de l'architecture GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques phrases simples\n",
    "sentences = [\n",
    "    \"la goutte d'eau qui fait déborder le vase\", \n",
    "    \"Il n'y a pas de fumée sans feu\", \n",
    "    \"Il faut battre le fer tant qu'il est chaud\", \n",
    "    \"Il ne faut pas mettre tous ses oeufs dans le même panier\", \n",
    "    \"Il faut tourner sept fois sa langue dans sa bouche avant de parler\", \n",
    "    \"L'habit ne fait pas le moine\", \n",
    "    \"Il ne faut pas réveiller le chat qui dort\", \n",
    "    \"Il faut se méfier de l'eau qui dort\", \n",
    "    \"C'est l'hôpital qui se moque de la charité\", \n",
    "    \"Qui vole un oeuf vole un boeuf\", \n",
    "    \"Chercher midi à quatorze heures\", \n",
    "    \"Avoir un poil dans la main\", \n",
    "    \"Être dans de beaux draps\", \n",
    "    \"Avoir la tête dans les nuages\", \n",
    "    \"Mettre les pieds dans le plat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Création du vocabulaire \n",
    "Premièrement, on analyse tous les textes pour récupérer l'ensembles des mots utilisés et on numérote les tokens (mots), chaque mot a ainsi un indice unique.\n",
    "\n",
    "Un objet Tokenizer sera utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb de mots différents rencontrés : 72\n",
      "le: 1, il: 2, dans: 3, qui: 4, de: 5, faut: 6, la: 7, pas: 8, ne: 9, un: 10, \n"
     ]
    }
   ],
   "source": [
    "# Récupération des tokens : le Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "#tokenizer.word_index est un dictionnaire (une map), on le transforme en liste de mots (liste[0] = mot 1 (0+1))  \n",
    "liste = list(tokenizer.word_index.keys())\n",
    "\n",
    "print(\"nb de mots différents rencontrés :\", total_words)\n",
    "for key, value in islice(tokenizer.word_index.items(), 10):\n",
    "    print(f\"{key}: {value}\", end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Transformer un texte en vecteur\n",
    "Maintenant, on remplace chaque mot, token, par son indice pour créer 1 vecteur d'entiers à partir d'une chaîne de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la goutte d'eau qui fait déborder le vase\n",
      "est traduit en\n",
      "[7, 19, 20, 4, 11, 21, 1, 22]\n"
     ]
    }
   ],
   "source": [
    "phrase0 = sentences[0]\n",
    "vecteur0 = tokenizer.texts_to_sequences([phrase0])[0]\n",
    "print(phrase0)\n",
    "print(\"est traduit en\")\n",
    "print(vecteur0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on souhaite apprendre progressivement la suite d'une phrase : \n",
    "- la goutte \n",
    "- la goutte d'eau \n",
    "- la goutte d'eau qui \n",
    "- la goutte d'eau qui fait\n",
    "- la goutte d'eau qui fait déborder \n",
    "- la goutte d'eau qui fait déborder le \n",
    "- la goutte d'eau qui fait déborder le vase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des textes en vecteurs \n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 19],\n",
       " [7, 19, 20],\n",
       " [7, 19, 20, 4],\n",
       " [7, 19, 20, 4, 11],\n",
       " [7, 19, 20, 4, 11, 21]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrage des vecteurs pour qu'ils aient tous la même longueur\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "# on rempli les cases \"pre\"cedentes de 0 si la longueur du vecteur < max_sequence_len\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la phrase ' la goutte d'eau qui fait déborder le vase ' est traduite en plusieurs vecteurs de même taille :\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  7 19] -> 'la goutte '\n",
      "[ 0  0  0  0  0  0  0  0  0  0  7 19 20] -> 'la goutte d'eau '\n",
      "[ 0  0  0  0  0  0  0  0  0  7 19 20  4] -> 'la goutte d'eau qui '\n",
      "[ 0  0  0  0  0  0  0  0  7 19 20  4 11] -> 'la goutte d'eau qui fait '\n",
      "[ 0  0  0  0  0  0  0  7 19 20  4 11 21] -> 'la goutte d'eau qui fait déborder '\n",
      "[ 0  0  0  0  0  0  7 19 20  4 11 21  1] -> 'la goutte d'eau qui fait déborder le '\n"
     ]
    }
   ],
   "source": [
    "print(\"la phrase '\", sentences[0], \"' est traduite en plusieurs vecteurs de même taille :\")\n",
    "split = sentences[0].split()\n",
    "for i in range(6):\n",
    "    print(input_sequences[i], end=\" -> '\")\n",
    "    for j in range(i+2):\n",
    "        print(split[j], end=\" \")\n",
    "    print(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creer les x (les  valeurs de chaque vecteur sauf la dernière)\n",
    "X = input_sequences[:, :-1]\n",
    "# creer les y (la derniere valeur de chaque vecteur)\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaque mot de sortie est représenté par un vecteur de 0s,  seul l'indice du mot = 1 \n",
    "#donc le vecteur est aussi grand que le nb de mots trouvés\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Le modèle de réseau\n",
    "Spécialement pour le texte, on décide de représenter un mot, non plus par un entier, mais par un vecteur de caractéristiques qui le représente.\n",
    "\n",
    "La couche **Embedding** permet cette transformation. \n",
    "- Théoriquement, si chat=2 et chien=5, un embedding de taille 3 donnera chat=[0.1, -0.4, 0.3] et chien=[0.5, -0.2, 0.5]\n",
    "- Ces vecteurs s'affine en fonction de l'apprentissage du texte.. A priori, si chien et chat sont utilisés de manière identique 'le chien mange\", \"le chat mange\", ..... au bout d'un certain temps, les vecteurs chat et chien vont posséder des valeurs semblables.\n",
    "\n",
    "La couche **GRU** est chargée de l'apprentissage des séries de valeurs. On peut utiliser plusieurs couches GRU, return_sequences sera vrai pour toutes sauf la dernière. La taille de la couche est d'environ 100 pour un vocabulaire moyen (qq milliers de mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du model\n",
    "model = Sequential()\n",
    "## Embedding : chaque mot est représenté par un vecteur de 50 valeurs\n",
    "model.add(Embedding(total_words, 50))\n",
    "model.add(GRU(100, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patienter 30s pendant l'entrainement...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23c5be17b30>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# entrainer le modele\n",
    "print(\"patienter 30s pendant l'entrainement...\")\n",
    "model.fit(X, y, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Prédictions\n",
    "Pour compléter une phrase, on demande la génération d'un mot, puis la génération d'un nouveau mot complétant la phrase à laquelle on a ajouté le mot précédent, etc. jusqu'au nombre de mots souhaités.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour prédire le mot suivant\n",
    "def predict_next_word(start_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([start_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                start_text += \" \" + word\n",
    "                break\n",
    "    \n",
    "    return start_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first part : il faut tourner sept fois sa  \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Prediction: il faut tourner sept fois sa   langue dans sa\n",
      "--------------------------------------------------\n",
      "first part : la goutte d'eau qui fait  \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Prediction: la goutte d'eau qui fait   déborder le vase\n",
      "--------------------------------------------------\n",
      "first part : Qui vole un oeuf   \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Prediction: Qui vole un oeuf    vole un boeuf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tester le modèle avec quelques textes\n",
    "start_texts = [\n",
    "    \"il faut tourner sept fois sa  \",\n",
    "    \"la goutte d'eau qui fait  \",\n",
    "    \"Qui vole un oeuf   \",\n",
    "]\n",
    "\n",
    "for text in start_texts:\n",
    "    print(f\"first part : {text}\")\n",
    "    nb = 5 if text.count(\" \")<5 else 3\n",
    "    print(f\"prédictions de {nb} mots.\")\n",
    "    print(f\"Prediction: {predict_next_word(text, nb)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first part : chercher un poil   \n",
      "prédictions de 4 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Prediction: chercher un poil    dans la main vole\n",
      "--------------------------------------------------\n",
      "first part : il faut battre le moine   \n",
      "prédictions de 4 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Prediction: il faut battre le moine    tant qu'il est chaud\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tester le modèle avec quelques textes\n",
    "start_texts = [\n",
    "    \"chercher un poil   \",\n",
    "    \"il faut battre le moine   \",\n",
    "]\n",
    "\n",
    "for text in start_texts:\n",
    "    print(f\"first part : {text}\")\n",
    "    nb = 4 #if text.count(\" \")<5 else 3\n",
    "    print(f\"prédictions de {nb} mots.\")\n",
    "    print(f\"Prediction: {predict_next_word(text, nb)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Post traitement\n",
    "On s'aperçoit de petit bugs, dit \"halunications\". \n",
    "Il faut souvent vérifier les sorties et les corriger (par exemple, empêcher la répétition du même mot, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
