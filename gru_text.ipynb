{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple de réseaux récurent prédisant du texte\n",
    "## Utilisation de l'architecture GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques phrases simples\n",
    "sentences = [\n",
    "    \"la goutte d'eau qui fait déborder le vase\", \n",
    "    \"Il n'y a pas de fumée sans feu\", \n",
    "    \"Il faut battre le fer tant qu'il est chaud\", \n",
    "    \"Il ne faut pas mettre tous ses oeufs dans le même panier\", \n",
    "    \"Il faut tourner sept fois sa langue dans sa bouche avant de parler\", \n",
    "    \"L'habit ne fait pas le moine\", \n",
    "    \"Il ne faut pas réveiller le chat qui dort\", \n",
    "    \"Il faut se méfier de l'eau qui dort\", \n",
    "    \"C'est l'hôpital qui se moque de la charité\", \n",
    "    \"Qui vole un oeuf vole un boeuf\", \n",
    "    \"Chercher midi à quatorze heures\", \n",
    "    \"Avoir un poil dans la main\", \n",
    "    \"Être dans de beaux draps\", \n",
    "    \"Avoir la tête dans les nuages\", \n",
    "    \"Mettre les pieds dans le plat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb de mots différents rencontrés : 72\n"
     ]
    }
   ],
   "source": [
    "# analyse du texte\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(\"nb de mots différents rencontrés :\", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voici les premiers mots trouvés : \n",
      "(1:'le'), (2:'il'), (3:'dans'), (4:'qui'), (5:'de'), (6:'faut'), (7:'la'), (8:'pas'), (9:'ne'), (10:'un'), \n"
     ]
    }
   ],
   "source": [
    "type(tokenizer.word_index)\n",
    "#from dict to list \n",
    "liste = list(tokenizer.word_index.keys())\n",
    "print(\"voici les premiers mots trouvés : \")\n",
    "for i in range(10): print(f\"({i+1}:'{liste[i]}')\", end= \", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des textes en vecteurs \n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrage des vecteurs pour qu'ils aient tous la même longueur\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la phrase ' la goutte d'eau qui fait déborder le vase ' est traduite en plusieurs vecteurs :\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  7 19] -> 'la goutte '\n",
      "[ 0  0  0  0  0  0  0  0  0  0  7 19 20] -> 'la goutte d'eau '\n",
      "[ 0  0  0  0  0  0  0  0  0  7 19 20  4] -> 'la goutte d'eau qui '\n",
      "[ 0  0  0  0  0  0  0  0  7 19 20  4 11] -> 'la goutte d'eau qui fait '\n",
      "[ 0  0  0  0  0  0  0  7 19 20  4 11 21] -> 'la goutte d'eau qui fait déborder '\n",
      "[ 0  0  0  0  0  0  7 19 20  4 11 21  1] -> 'la goutte d'eau qui fait déborder le '\n"
     ]
    }
   ],
   "source": [
    "print(\"la phrase '\", sentences[0], \"' est traduite en plusieurs vecteurs :\")\n",
    "split = sentences[0].split()\n",
    "for i in range(6):\n",
    "    print(input_sequences[i], end=\" -> '\")\n",
    "    for j in range(i+2):\n",
    "        print(split[j], end=\" \")\n",
    "    print(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creer les x (premieres valeurs de chaque vecteur)\n",
    "X = input_sequences[:, :-1]\n",
    "# creer les y (derniere valeur de chaque vecteur)\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaque mot de sortie est représenté par un vecteur de 0, avec 1 correspondant à l'indice du mot\n",
    "#donc le vecteur est aussi grand que le nb de mots trouvés\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#creation du model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 50, input_length=max_sequence_len-1))\n",
    "model.add(GRU(120, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patienter 30s pendant l'entrainement...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24b9ca5e780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# entrainer le modele\n",
    "print(\"patienter 30s pendant l'entrainement...\")\n",
    "model.fit(X, y, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour prédire le mot suivant\n",
    "def predict_next_word(start_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([start_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                start_text += \" \" + word\n",
    "                break\n",
    "    \n",
    "    return start_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first part : il faut tourner sept fois sa  \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Prediction: il faut tourner sept fois sa   langue dans sa\n",
      "--------------------------------------------------\n",
      "first part : la goutte d'eau qui fait  \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Prediction: la goutte d'eau qui fait   déborder le vase\n",
      "--------------------------------------------------\n",
      "first part : Qui vole un oeuf   \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediction: Qui vole un oeuf    vole un boeuf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tester le modèle avec quelques textes\n",
    "start_texts = [\n",
    "    \"il faut tourner sept fois sa  \",\n",
    "    \"la goutte d'eau qui fait  \",\n",
    "    \"Qui vole un oeuf   \",\n",
    "]\n",
    "\n",
    "for text in start_texts:\n",
    "    print(f\"first part : {text}\")\n",
    "    nb = 5 if text.count(\" \")<5 else 3\n",
    "    print(f\"prédictions de {nb} mots.\")\n",
    "    print(f\"Prediction: {predict_next_word(text, nb)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first part : le boeuf ne fait pas   \n",
      "prédictions de 5 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Prediction: le boeuf ne fait pas    le moine tant qu'il est\n",
      "--------------------------------------------------\n",
      "first part : il faut battre le moine   \n",
      "prédictions de 5 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Prediction: il faut battre le moine    tant qu'il est chaud chaud\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tester le modèle avec quelques textes\n",
    "start_texts = [\n",
    "    \"le boeuf ne fait pas   \",\n",
    "    \"il faut battre le moine   \",\n",
    "]\n",
    "\n",
    "for text in start_texts:\n",
    "    print(f\"first part : {text}\")\n",
    "    nb = 5 #if text.count(\" \")<5 else 3\n",
    "    print(f\"prédictions de {nb} mots.\")\n",
    "    print(f\"Prediction: {predict_next_word(text, nb)}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
