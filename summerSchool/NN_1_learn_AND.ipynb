{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://github.com/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/summerSchool/NN_1_learn_AND.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qF8xmpAdACr"
   },
   "source": [
    "# Learning AND with PyTorch\n",
    "## Illustration of the Importance of Bias\n",
    "\n",
    "|a|b|a and b|\n",
    "|:-:|:-:|:-:|\n",
    "|0|0|0|\n",
    "|0|1|0|\n",
    "|1|0|0|\n",
    "|1|1|1|\n",
    "\n",
    "*Theoretically, learning AND with a single-layer neural network is not possible.*\n",
    "\n",
    "Indeed, the layer consists of only 1 neuron (1 output), its inputs are the values $a$ and $b$.<br>\n",
    "$w_a$ and $w_b$ being the weights assigned to these values, we need to verify:\n",
    " - $f(0) \\searrow 0$ --> ok\n",
    " - $f(w_b) \\searrow 0$, $\\Rightarrow w_b \\leq 0$\n",
    " - $f(w_a) \\searrow 0$ $\\Rightarrow w_a \\leq 0$\n",
    " - $f(w_a + w_b) \\nearrow 1$ $\\Rightarrow w_a + w_b \\geq 1$ --> conflict with the previous lines\n",
    "\n",
    "*Let's verify this...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZpJB9iOdRS3"
   },
   "source": [
    "---\n",
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odDg9S7xdFsv"
   },
   "outputs": [],
   "source": [
    "# import the torch library\n",
    "import torch\n",
    "# import the Neural Network class\n",
    "import torch.nn as nn\n",
    "# import the optimizers class\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XN8WXXdTdXx0"
   },
   "source": [
    "---\n",
    "\n",
    "## Define Inputs and Expected Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1mjGN2Bb5Ki"
   },
   "outputs": [],
   "source": [
    "# a and b are the only inputs\n",
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]], float)\n",
    "\n",
    "# single output\n",
    "outputs = np.array([[0],[0],[0],[1]], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform inputs and outputs into tensors\n",
    "tensor_X = torch.tensor(inputs, dtype=torch.float32)\n",
    "tensor_y = torch.tensor(outputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC-1MnShdwe0"
   },
   "source": [
    "---\n",
    "## 1. Version without BIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Choose the Network Model\n",
    "***Here the layers are sequential***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbST4EmqcCdJ"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jo5Ej8kkd8nh"
   },
   "source": [
    "### 1.2. Define the Network Architecture\n",
    "- Here, a single layer consisting of 1 output neuron,\n",
    "- 2 input neurons (for each value),\n",
    "- using the sigmoid as the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vr6dcOJicGYM"
   },
   "outputs": [],
   "source": [
    "# add a linear layer with 2 inputs and 1 output, without bias\n",
    "model.add_module('single layer', nn.Linear(2, 1, bias=False))\n",
    "# add a sigmoid activation function\n",
    "model.add_module('sigmoid', nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8L4LxSBWeaF1"
   },
   "source": [
    "---\n",
    "\n",
    "### 1.3. Error Correction\n",
    "Here, we specify that\n",
    " - the error correction algorithm is 'Adam',\n",
    " - the calculated error is the mean of squared errors. <br>\n",
    " $E = \\Sigma_{i=1 \\dots n}{(y^I_i - y_i)} / n$, $y^I_i$ ideal expected output, $y_i$ calculated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlnG97g7cQKW"
   },
   "outputs": [],
   "source": [
    "# use the mean squared error function\n",
    "criterion = nn.MSELoss()\n",
    "# use the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZ4IQ-bIdtN1"
   },
   "source": [
    "---\n",
    "\n",
    "### 1.4. Train the Network\n",
    "- Here we do not display the training steps,\n",
    "- and we run 2000 training cycles (wait between 4 to 6 minutes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ddTla-J_cfz8",
    "outputId": "3f251574-0acf-4b59-e480-3e736b427625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.2500\n",
      "Epoch [200/100], Loss: 0.2500\n",
      "Epoch [300/100], Loss: 0.2500\n",
      "Epoch [400/100], Loss: 0.2500\n",
      "Epoch [500/100], Loss: 0.2500\n",
      "Epoch [600/100], Loss: 0.2500\n",
      "Epoch [700/100], Loss: 0.2500\n",
      "Epoch [800/100], Loss: 0.2500\n",
      "Epoch [900/100], Loss: 0.2500\n",
      "Epoch [1000/100], Loss: 0.2500\n",
      "Epoch [1100/100], Loss: 0.2500\n",
      "Epoch [1200/100], Loss: 0.2500\n",
      "Epoch [1300/100], Loss: 0.2500\n",
      "Epoch [1400/100], Loss: 0.2500\n",
      "Epoch [1500/100], Loss: 0.2500\n",
      "Epoch [1600/100], Loss: 0.2500\n",
      "Epoch [1700/100], Loss: 0.2500\n",
      "Epoch [1800/100], Loss: 0.2500\n",
      "Epoch [1900/100], Loss: 0.2500\n",
      "Epoch [2000/100], Loss: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# launch training for 2000 iterations\n",
    "model.train()  # set the model to training mode\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # calculate outputs and loss\n",
    "    computed_outputs = model(tensor_X)\n",
    "    loss = criterion(computed_outputs, tensor_y)\n",
    "\n",
    "    # backpropagation and optimization\n",
    "    # reset gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # for demo, display the loss every 10 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0zBYIALlDYp"
   },
   "source": [
    "---\n",
    "\n",
    "### 1.5. Verify the Network\n",
    "Optional step, generally ***we test the network on other examples***.\n",
    "- Here, we don't have any. So we ask it to calculate the output for each example in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (single layer): Linear(in_features=2, out_features=1, bias=False)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z89vptAXcuDM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "Input: [0. 0.], Predicted: 0.50, Expected: 0.00\n",
      "Input: [0. 1.], Predicted: 0.50, Expected: 0.00\n",
      "Input: [1. 0.], Predicted: 0.50, Expected: 0.00\n",
      "Input: [1. 1.], Predicted: 0.50, Expected: 1.00\n",
      "Final Loss: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions from inputs\n",
    "# use torch.no_grad() to not modify gradients\n",
    "with torch.no_grad():\n",
    "    predictions = model(tensor_X)\n",
    "    print(\"Predictions:\")\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(f\"Input: {inputs[i]}, Predicted: {pred.item():.2f}, Expected: {outputs[i][0]:.2f}\")\n",
    "\n",
    "# display the final loss\n",
    "final_loss = criterion(predictions, tensor_y)\n",
    "print(f'Final Loss: {final_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big errors here! As expected without the bias..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "E7Lluiifcx0b",
    "outputId": "a68a8d3f-24f5-4f70-95fc-a215b158ee09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "single layer.weight: [[-1.1913059e-08 -5.5206133e-08]]\n"
     ]
    }
   ],
   "source": [
    "# display the model weights\n",
    "print(\"Weights:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Version WITH BIAS\n",
    "\n",
    "The table is then\n",
    "\n",
    "|bias|a|b|a and b|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|1|0|0|0|\n",
    "|1|0|1|0|\n",
    "|1|1|0|0|\n",
    "|1|1|1|1|\n",
    "\n",
    "*Theoretically, learning AND with a single-layer neural network is then possible.*\n",
    "\n",
    "Indeed, the layer consists of only 1 neuron (1 output), its inputs are the values `bias`, `a`, and `b`.\n",
    "`wbias`, `wa`, and `wb` being the weights assigned to these values, we need to verify:\n",
    " - $f(w_{bias}) \\searrow 0$ --> ok\n",
    " - $f(w_{bias} + w_b) \\searrow 0$\n",
    " - $f(w_{bias} + w_a) \\searrow 0$\n",
    " - $f(w_{bias} + w_a + w_b) \\nearrow 1$ --> possible if $w_a \\leq -w_{bias}$ and $w_b \\leq -w_{bias}$, if $w_a \\geq 1$ and $w_b \\geq 1$ and if $w_{bias} + w_a + w_b \\geq 1$\n",
    "\n",
    "*Let's verify this...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.1. Define the Network Architecture\n",
    "- Here, a single layer consisting of 1 output neuron,\n",
    "- 3 input neurons (2 containing the values + **a Bias** (always emitting the signal 1)),\n",
    "- using the sigmoid as the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "# add a linear layer with 2 inputs and 1 output, with bias (default)\n",
    "model.add_module('single layer', nn.Linear(2, 1))\n",
    "# add a sigmoid activation function\n",
    "model.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "# use the mean squared error function\n",
    "criterion = nn.MSELoss()\n",
    "# use the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2. Train the Network\n",
    "We keep the same optimizer and the same loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.1402\n",
      "Epoch [200/100], Loss: 0.0958\n",
      "Epoch [300/100], Loss: 0.0706\n",
      "Epoch [400/100], Loss: 0.0540\n",
      "Epoch [500/100], Loss: 0.0423\n",
      "Epoch [600/100], Loss: 0.0338\n",
      "Epoch [700/100], Loss: 0.0274\n",
      "Epoch [800/100], Loss: 0.0226\n",
      "Epoch [900/100], Loss: 0.0188\n",
      "Epoch [1000/100], Loss: 0.0159\n",
      "Epoch [1100/100], Loss: 0.0136\n",
      "Epoch [1200/100], Loss: 0.0117\n",
      "Epoch [1300/100], Loss: 0.0101\n",
      "Epoch [1400/100], Loss: 0.0089\n",
      "Epoch [1500/100], Loss: 0.0078\n",
      "Epoch [1600/100], Loss: 0.0069\n",
      "Epoch [1700/100], Loss: 0.0062\n",
      "Epoch [1800/100], Loss: 0.0055\n",
      "Epoch [1900/100], Loss: 0.0050\n",
      "Epoch [2000/100], Loss: 0.0045\n"
     ]
    }
   ],
   "source": [
    "# launch training for 1000 iterations\n",
    "model.train()  # set the model to training mode\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # calculate outputs and loss\n",
    "    computed_outputs = model(tensor_X)\n",
    "    loss = criterion(computed_outputs, tensor_y)\n",
    "\n",
    "    # backpropagation and optimization\n",
    "    # reset gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # for demo, display the loss every 10 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2. Verify the Network\n",
    "- We calculate the output for each example in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "Input: [0. 0.], Predicted: 0.00, Expected: 0.00\n",
      "Input: [0. 1.], Predicted: 0.08, Expected: 0.00\n",
      "Input: [1. 0.], Predicted: 0.07, Expected: 0.00\n",
      "Input: [1. 1.], Predicted: 0.92, Expected: 1.00\n",
      "Final Loss: 0.0045\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set the model to evaluation mode\n",
    "# calculate predictions from inputs\n",
    "# use torch.no_grad() to not modify gradients\n",
    "with torch.no_grad():\n",
    "    predictions = model(tensor_X)\n",
    "    print(\"Predictions:\")\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(f\"Input: {inputs[i]}, Predicted: {pred.item():.2f}, Expected: {outputs[i][0]:.2f}\")\n",
    "\n",
    "# display the final loss\n",
    "final_loss = criterion(predictions, tensor_y)\n",
    "print(f'Final Loss: {final_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Almost perfect learning!!!**\n",
    "- -> concrete demonstration of the effect of `Bias`!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "single layer.weight: [[4.9143333 4.978275 ]]\n",
      "single layer.bias: [-7.4425335]\n"
     ]
    }
   ],
   "source": [
    "# display the model weights\n",
    "print(\"Weights:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Usage\n",
    "Let's test other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = np.array([[0.5,0.2], [0.9, 0.7], [0.1, 0.9]], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how the model predicts the results for new inputs\n",
    "# use torch.no_grad() to not modify gradients\n",
    "with torch.no_grad(): predictions = model(torch.tensor(tests, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(inputs, outputs):\n",
    "    for i in range(0, len(inputs)):\n",
    "        print(f\"If A is true at {inputs[i][0]*100:.2f}% and B true at {inputs[i][1]*100:.2f}%, having A and B true at the same time is possible at {outputs[i][0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If A is true at 50.00% and B true at 20.00%, having A and B true at the same time is possible at 1.82%\n",
      "If A is true at 90.00% and B true at 70.00%, having A and B true at the same time is possible at 61.42%\n",
      "If A is true at 10.00% and B true at 90.00%, having A and B true at the same time is possible at 7.79%\n"
     ]
    }
   ],
   "source": [
    "print_predictions(tests, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network can **interpolate outputs** from unlearned entries !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TestET.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
