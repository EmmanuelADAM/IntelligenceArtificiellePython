{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/summerSchool/NN_6_gru_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Recurrent Networks Predicting Text\n",
    "## Using the GRU Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/emmanuel adam/Documents/GitHub/IntelligenceArtificiellePython/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Few Simple Sentences\n",
    "\n",
    "sentences_fr = [\n",
    "    \"la goutte d'eau qui fait déborder le vase\", \n",
    "    \"Il n'y a pas de fumée sans feu\", \n",
    "    \"Il faut battre le fer tant qu'il est chaud\", \n",
    "    \"Il ne faut pas mettre tous ses oeufs dans le même panier\", \n",
    "    \"Il faut tourner sept fois sa langue dans sa bouche avant de parler\", \n",
    "    \"L'habit ne fait pas le moine\", \n",
    "    \"Il ne faut pas réveiller le chat qui dort\", \n",
    "    \"Il faut se méfier de l'eau qui dort\", \n",
    "    \"C'est l'hôpital qui se moque de la charité\", \n",
    "    \"Qui vole un oeuf vole un boeuf\", \n",
    "    \"Chercher midi à quatorze heures\", \n",
    "    \"Avoir un poil dans la main\", \n",
    "    \"Être dans de beaux draps\", \n",
    "    \"Avoir la tête dans les nuages\", \n",
    "    \"Mettre les pieds dans le plat\"]\n",
    "sentences = [\n",
    "    \"the straw that broke the camel's back\",\n",
    "    \"there's no smoke without fire\",\n",
    "    \"strike while the iron is hot\",\n",
    "    \"don't put all your eggs in one basket\",\n",
    "    \"think before you speak\",\n",
    "    \"Clothes don't make the man\",\n",
    "    \"let sleeping dogs lie\",\n",
    "    \"still waters run deep\",\n",
    "    \"the pot calling the kettle black\",\n",
    "    \"Steal an egg, steal an ox\",\n",
    "    \"make a mountain out of a molehill\",\n",
    "    \"to be in a fine mess\",\n",
    "    \"to have your head in the clouds\",\n",
    "    \"put your foot in your mouth\"]\n",
    "# test other sentences, other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Creating the Vocabulary\n",
    "First, we analyze all the texts to collect all the words used and assign a number to each token (word). Each word thus has a unique index.\n",
    "\n",
    "A Tokenizer object will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get the tokens :  Tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m()\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(sentences)\n\u001b[0;32m      4\u001b[0m total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# get the tokens :  Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "#tokenizer.word_index is a dictionary ( map), we have to transform it in list of words  (liste[0] = mot 1 (0+1))  \n",
    "liste = list(tokenizer.word_index.keys())\n",
    "\n",
    "print(\"nb of differents words read :\", total_words)\n",
    "for key, value in islice(tokenizer.word_index.items(), 10):\n",
    "    print(f\"{key}: {value}\", end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Transforming Text into a Vector\n",
    "Now, we replace each word or token with its index to create one integer vector from a character string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la goutte d'eau qui fait déborder le vase\n",
      "est traduit en\n",
      "[7, 19, 20, 4, 11, 21, 1, 22]\n"
     ]
    }
   ],
   "source": [
    "phrase0 = sentences[0]\n",
    "vecteur0 = tokenizer.texts_to_sequences([phrase0])[0]\n",
    "print(phrase0)\n",
    "print(\"is traduced in:\")\n",
    "print(vecteur0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to gradually learn the continuation of a sentence:\n",
    "- the drop\n",
    "- the drop of water\n",
    "- the drop of water that\n",
    "- the drop of water that makes\n",
    "- the drop of water that makes the\n",
    "- the drop of water that makes the vase\n",
    "- the drop of water that makes the vase overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from texts to vectors \n",
    "input_sequences = []\n",
    "for sentence in sentences:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 19],\n",
       " [7, 19, 20],\n",
       " [7, 19, 20, 4],\n",
       " [7, 19, 20, 4, 11],\n",
       " [7, 19, 20, 4, 11, 21]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrating the vectors so that they all have the same length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "# We fill the preceding positions with 0 if the length of the vector < max_sequence_len.\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la phrase ' la goutte d'eau qui fait déborder le vase ' est traduite en plusieurs vecteurs de même taille :\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  7 19] -> 'la goutte '\n",
      "[ 0  0  0  0  0  0  0  0  0  0  7 19 20] -> 'la goutte d'eau '\n",
      "[ 0  0  0  0  0  0  0  0  0  7 19 20  4] -> 'la goutte d'eau qui '\n",
      "[ 0  0  0  0  0  0  0  0  7 19 20  4 11] -> 'la goutte d'eau qui fait '\n",
      "[ 0  0  0  0  0  0  0  7 19 20  4 11 21] -> 'la goutte d'eau qui fait déborder '\n",
      "[ 0  0  0  0  0  0  7 19 20  4 11 21  1] -> 'la goutte d'eau qui fait déborder le '\n"
     ]
    }
   ],
   "source": [
    "print(\"la phrase '\", sentences[0], \"' is translated into multiple vectors of the same size:\")\n",
    "split = sentences[0].split()\n",
    "for i in range(6):\n",
    "    print(input_sequences[i], end=\" -> '\")\n",
    "    for j in range(i+2):\n",
    "        print(split[j], end=\" \")\n",
    "    print(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Xs  (vector values except the last one)\n",
    "X = input_sequences[:, :-1]\n",
    "# creer les y (last value of the vector)\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each output word is represented by a vector of 0s, only the index of the word is set to 1\n",
    "# Therefore, the vector is as large as the number of words found\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Network Model\n",
    "Specifically for text, we decide to represent a word not by an integer, but by a feature vector that represents it.\n",
    "\n",
    "The **Embedding** layer allows this transformation.\n",
    "\n",
    "Theoretically, if \"chat\" (cat) = 2 and \"chien\" (dog) = 5, an embedding of size 3 will give \"chat\" = [0.1, -0.4, 0.3] and \"chien\" = [0.5, -0.2, 0.5].\n",
    "\n",
    "These vectors are refined based on the learning of the text. A priori, if \"chien\" and \"chat\" are used identically, as in \"le chien mange\" (the dog eats), \"le chat mange\" (the cat eats), after some time, the vectors for \"chat\" and \"chien\" will have similar values.\n",
    "\n",
    "The **GRU** layer is responsible for learning sequences of values. Multiple GRU layers can be used, with return_sequences being true for all layers except the last. The size of the layer is approximately 100 for an average vocabulary (a few thousand words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation\n",
    "model = Sequential()\n",
    "## Embedding : each word is represented by a vector of 50 values\n",
    "model.add(Embedding(total_words, 50))\n",
    "model.add(GRU(100, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patienter 30s pendant l'entrainement...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23c5be17b30>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# train the modele\n",
    "print(\"wait few seconds when training...\")\n",
    "model.fit(X, y, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Predictions\n",
    "To complete a sentence, we request the generation of a word, then the generation of a new word that completes the sentence to which we have added the previous word, and so on, until the desired number of words is reached.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the next word\n",
    "def predict_next_word(start_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([start_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        #take the most probable word\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                start_text += \" \" + word\n",
    "                break\n",
    "    \n",
    "    return start_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first part : il faut tourner sept fois sa  \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Prediction: il faut tourner sept fois sa   langue dans sa\n",
      "--------------------------------------------------\n",
      "first part : la goutte d'eau qui fait  \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Prediction: la goutte d'eau qui fait   déborder le vase\n",
      "--------------------------------------------------\n",
      "first part : Qui vole un oeuf   \n",
      "prédictions de 3 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Prediction: Qui vole un oeuf    vole un boeuf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model with different sentences\n",
    "start_texts = [\n",
    "    \"don't put all your eggs  \",\n",
    "    \"make a mountain  \",\n",
    "    \"to have your head   \",\n",
    "]\n",
    "\n",
    "for text in start_texts:\n",
    "    print(f\"first part : {text}\")\n",
    "    nb = 5 if text.count(\" \")<5 else 3\n",
    "    print(f\"prédictions de {nb} mots.\")\n",
    "    print(f\"Prediction: {predict_next_word(text, nb)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first part : chercher un poil   \n",
      "prédictions de 4 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Prediction: chercher un poil    dans la main vole\n",
      "--------------------------------------------------\n",
      "first part : il faut battre le moine   \n",
      "prédictions de 4 mots.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Prediction: il faut battre le moine    tant qu'il est chaud\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# other additional tests\n",
    "start_texts = [\n",
    "    \"to have your egg \",\n",
    "    \"when there moutains \",\n",
    "]\n",
    "\n",
    "for text in start_texts:\n",
    "    print(f\"first part : {text}\")\n",
    "    nb = 4 #if text.count(\" \")<5 else 3\n",
    "    print(f\"prédictions de {nb} mots.\")\n",
    "    print(f\"Prediction: {predict_next_word(text, nb)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Post-processing\n",
    "We notice small bugs, known as \"hallucinations.\" It is often necessary to check and correct the outputs (for example, preventing the repetition of the same word, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
