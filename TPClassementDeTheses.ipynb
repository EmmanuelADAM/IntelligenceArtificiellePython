{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPClassementDeTheses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVNE4BGIsOlY"
   },
   "source": [
    "# Classement automatique de textes par réseau de neurones\n",
    "\n",
    "L’objectif de ce TP est de développer un modèle prédictif pour la classification des thèses scientifiques (données textuelles) à partir d’un apprentissage automatique par réseau de neurones et d'une représentation en sac de mots (bag of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQ5vas82illV"
   },
   "source": [
    "## Ensemble de données \n",
    "Le tp utilisera un ensemble de données issues des sites de diffusions des thèses soutenues en France : https://tel.archives-ouvertes.fr/ & http://www.theses.fr/.\n",
    "\n",
    "Dans ce fichier ([TPClassementTheses.ods](https://github.com/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPClassementTheses.ods)), vous récupérerez des exemples de classements. \n",
    "Les titres sont réels, les noms des directeurs ont été modifiés. Des classements ont été posés; certains sont manquants.\n",
    "\n",
    "Ce répertoire contient donc des titres de thèses classées IA, d'autres non; certains titres n'ont pas de classements.\n",
    "\n",
    "90% des données classées sont à utiliser en tant que données d'entraînement et 10% seront utilisées en données de test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWFXrAtkvcg_"
   },
   "source": [
    "---\n",
    "Si vous utilisez colab, vous devrez charger vos fichiers de données dans votre Drive Google.\n",
    "\n",
    "Pour pouvoir les utiliser, il vous faut monter le répertoire drive pour qu'il soit accessible par colab.\n",
    "Exécutez le code suivant,  une clé vous sera demandée. Il vous suffit de suivre le lien, de sélectionner votre profile pour obtenir votre clé que vous copierez dans le champs prévu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "4TodTL602bBv",
    "outputId": "cbc50963-1484-4189-b87b-b0d7d60af79c"
   },
   "outputs": [],
   "source": [
    "#bloc à exécuter si vous utilisez colab\n",
    "from google.colab import drive, files\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IlL2ANmisdv"
   },
   "source": [
    "---\n",
    "## Préparation des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créations des fichiers de données\n",
    "\n",
    "A partir du fichier contenant les tables, vous devez créer 2 fichiers texte. \n",
    "L'un contenant les titres de thèses classées IA (theseIA.txt) et l'autre les titres de thèses non classées IA (theseNonIA.txt).\n",
    "\n",
    "Pour débuter, vous pouvez ne considérer qu'une seule feuille (data1) mais plus les fichiers d'exemples seront fournis, meilleur sera l'apprentissage.\n",
    "\n",
    "**Des titres ne sont pas classés.** Il arrive fréquemment que les données brutes soient incomplètes. Plusieurs solutions existe pour compléter les valeurs manquantes (valeur aléatoire, proportionnellement à la distribution existante, valeur fixe, ...) , ici on ignorera simplement les titres non classés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction des exemples\n",
    "Le nombre de mots utilisés le vocabulaire, étant assez vaste; on le réduit dans un premier temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiHdx0y1i8Bi"
   },
   "source": [
    "### Création du vocabulaire utile\n",
    "\n",
    "En chargeant l'ensemble des lignes contenues dans les fichiers texte, on crée un vocabulaire des mots utiles en :   \n",
    "  - identifiant les termes (**tokens**) entre espaces,\n",
    "  - supprimant toute ponctuation,\n",
    "  - supprimant tous les mots qui ne sont pas uniquement composés de caractères alphabétiques,\n",
    "  - supprimant tous les mots reconnus en tant que mots vides (stop words) (mots de liaison) français & anglais\n",
    "  - supprimant tous les mots dont la longueur est <= 2 caractères (sauf IA, AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement et nettoyage des données\n",
    "\n",
    "Il convient pour chaque fichier texte de créer le  \n",
    "  - identifier les termes (**tokens**) entre espaces,\n",
    "  - supprimer toute ponctuation,\n",
    "  - supprimer tous les mots qui ne sont pas uniquement composés de caractères alphabétiques,\n",
    "  - supprimer tous les mots reconnus en tant que mots vides (stop words) (mots de liaison) français & anglais\n",
    "  - supprimer tous les mots dont la longueur est <= 2 caractères (sauf IA, AI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLWxuYAjxPlo"
   },
   "source": [
    "### Les bibliothèques\n",
    "La préparation des données implique de pouvoir accéder au système de fichier, ainsi qu'à la bibliothèque nltk et aux bibliothèques spécialisées dans le traitement de texte.. : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSqBXmHvxnhF"
   },
   "outputs": [],
   "source": [
    "##import pour les fichiers et le traitement de données : \n",
    "from os import listdir\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "##import pour les réseaux de neurones : \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import preprocessing\n",
    "from numpy import array, zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "n6qZZ-VJw8Wy",
    "outputId": "1c8e7c95-f0ba-4f69-9914-c034c7f534cc"
   },
   "outputs": [],
   "source": [
    "rep = '/Users/.../.../' ## ICI VOTRE REPERTOIRE DE TRAVAIL\n",
    "# le code suivant charge l'ensemble des mots non importants \n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de lectures et d'\"épurage\" de fichier texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by0--ITCi-xa"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename:str)->list:\n",
    "    \"\"\"retourne les lignes de texte du fichier filename\"\"\"\n",
    "    # open the file as read only\n",
    "    with open(filename) as file:\n",
    "        text = file.read().splitlines() \n",
    "    return text\n",
    "\n",
    " \n",
    "def clean_doc(doc)->list:\n",
    "\n",
    "    \"\"\"retourne la liste de mots clés inclus dans le texte doc \n",
    "    qui ne font pas parti des stop_words anglais et francais\n",
    "    retire d autres mots cles comme 'vers', 'lors' \n",
    "    attention, ne doit pas oter les mots ia, ai, ..\"\"\"\n",
    "    tokens = doc.lower()\n",
    "    tokens = tokens.split()\n",
    "    # A CONTINUER .......!!!!!!\n",
    "    return tokens\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgALvdrLKOD3"
   },
   "source": [
    "### Tests de la récupération de mots clés d'un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "0CpapoCR0Hzs",
    "outputId": "23a01def-2b8c-4fef-e689-1289a90d2d61"
   },
   "outputs": [],
   "source": [
    "def test_recup():\n",
    "    # mots clés du fichier des theses IA\n",
    "    filename = rep + '/thesesIA.txt'\n",
    "    lines = load_doc(filename)\n",
    "    text = ' '.join(lines)\n",
    "    tokens = clean_doc(text)\n",
    "    print('les 10 premiers mots cles de ', filename)\n",
    "    print(tokens[:10])\n",
    "    # mots clés du fichier des theses NON IA\n",
    "    filename = rep + '/thesesNonIA.txt'\n",
    "    lines = load_doc(filename)\n",
    "    text = ' '.join(lines)\n",
    "    tokens = clean_doc(text)\n",
    "    print('les 10 premiers mots cles de ', filename)\n",
    "    print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yo75YEIjWIT"
   },
   "source": [
    "### Construire le vocabulaire global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Axlsvtg8jhAr"
   },
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filename, vocab):\n",
    "    \"\"\"cumule dans la liste vocab les mots du fichier filename \n",
    "    (1 seule occurence par mot dans vocab)\"\"\"\n",
    "    # load doc\n",
    "    lines = load_doc(filename)\n",
    "    doc = ' '.join(lines)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def create_global_vocabulary():\n",
    "    \"\"\"creer un vocabulaire (liste de mots clés associés à leurs occurences)\n",
    "    par la suite, un mot d'un texte ne faisant pas partie du vocabulaire ne sera\n",
    "    pas compte\"\"\"\n",
    "    vocab = Counter()\n",
    "    # ajouter les mots cles des repertoire pos et neg\n",
    "    add_doc_to_vocab(rep+'/thesesIA.txt', vocab)\n",
    "    add_doc_to_vocab(rep+'/thesesNonIA.txt', vocab)\n",
    "\n",
    "    # afficher le nb de mots cles trouves\n",
    "    print(\"nb de mots cles trouves dans les repertoires : \", len(vocab))\n",
    "    print(\"les 10 premiers mots cles du vocabulaire \\\n",
    "    (et leur nb d\\'apparition dans les exemples)  : \\n\", end='')\n",
    "    i=0\n",
    "    for (mot,count) in vocab.items(): \n",
    "        print(mot,':',count,end=\", \")\n",
    "        i = i+1\n",
    "        if i>10:break\n",
    "    # afficher les 10 mots cles les plus utilises\n",
    "    print(\"\\nles 10 mots cles les plus utilises : \", vocab.most_common(10))\n",
    "\n",
    "    # ne garder que les mots clés apparaissant au moins 2 fois\n",
    "    min_occurrence = 2\n",
    "    tokens = [token for (token,count) in vocab.items() if count >= min_occurrence]\n",
    "    print('en otant les mots utilise moins de ', min_occurrence, ' fois,', \n",
    "          ' nb de mots cles = ',len(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sauvegarde du vocabulaire\n",
    "def save_list(lines, filename):\n",
    "    \"\"\"sauve les mots de la liste lines dans le fichier filename\"\"\"\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    data = '\\n'.join(lines)\n",
    "    # write text\n",
    "    file.writelines(data)\n",
    "    # close file\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDFdsl97jq0t"
   },
   "outputs": [],
   "source": [
    "## A la 1ere utilisation, et a chaque modification des fichiers de données\n",
    "tokens = create_global_vocabulary()\n",
    "save_list(tokens, 'vocabThesesIA.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction des entrées\n",
    "Chaque entrée texte peut être maintenant simplifiée. \n",
    "Pour chaque entrée, on extrait les tokens qui ne sont pas présents dans le vocabulaire utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_line(line, vocab)->list:\n",
    "    \"\"\"retourne la liste des mots cles de la ligne appartenant au vocabulaire vocab\"\"\"\n",
    "    # clean line\n",
    "    tokens = clean_doc(line)    \n",
    "    # filter by vocab\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque fichier d'exemples, on en retire deux listes de lignes épurées, une liste contenant 90% des exemples, et une autre 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_file(filename, vocab)->list:\n",
    "    \"\"\"retourne deux listes des mots cles du repertoire directory; \n",
    "    la 1ere liste represente les 90% premieres lignes du fichier, \n",
    "    la 2nde represente les 10% restant\"\"\"\n",
    "    lines_firts = list()\n",
    "    lines_lasts = list()\n",
    "    i=1\n",
    "    lines = list()\n",
    "    # load and clean the file\n",
    "    with open(filename) as file:\n",
    "        lines = file.read().splitlines()\n",
    "    lines_filtered = [filter_line(line, vocab) for line in lines]\n",
    "    nb_train = int(len(lines_filtered) * 0.9)\n",
    "    lines_firts.extend(lines_filtered[:nb_train])\n",
    "    lines_lasts.extend(lines_filtered[nb_train:])\n",
    "    return (lines_firts,lines_lasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SI le vocabulaire n'a ete cree dans cette session mais avant ALORS le charger\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab)\n",
    "# load training and testing examples\n",
    "(positive_lines_train, positive_lines_test) = process_train_file(rep+'/thesesIA.txt', vocab)\n",
    "(negative_lines_train, negative_lines_test) = process_train_file(rep+'/thesesNonIA.txt', vocab)\n",
    "# summarize what we have\n",
    "print(\"nb exemples d'entrainement positifs : \", len(positive_lines_train))\n",
    "print(\"nb exemples d'entrainement negatifs : \", len(negative_lines_train))\n",
    "print(\"nb exemples de tests positifs : \", len(positive_lines_test))\n",
    "print(\"nb exemples de tests negatifs : \", len(negative_lines_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2oK8x7bj5CU"
   },
   "source": [
    "---\n",
    "## Représentation en sac de mots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdd9RpBskGUy"
   },
   "source": [
    "### Convertir les exemples en vecteurs\n",
    "Chaque exemple doit être transformé dans un vecteur de même format.\n",
    "On recrée un vocabulaire des mots présents dans les exemples; puis pour chaque exemple, on crée un vecteur de 0, 1 (1 si le ie mot du vocabulaire existe dans l'exemple).\n",
    "\n",
    "Ainsi tous les exemples de ligne sont transformés en vecteurs de même dimension.\n",
    "\n",
    "On utilise pour cela un \"Tokenizer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "# build the training doc based on training data\n",
    "training_doc = list()\n",
    "training_doc.extend(positive_lines_train)\n",
    "training_doc.extend(negative_lines_train)\n",
    "# ask to the tokenizer to build the bag of words : a set of (word, use)*\n",
    "# attention ici on ne demande pas la frequence d'apparition mais la presence ou non du mot du vocabulaire dans les lignes\n",
    "#-> tokenizer.fit_on_texts(A COMPLETER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Il ne reste plus qu'à\" demander au 'Tokenizer' de réaliser la transformation des exemples d'entraînement en vecteurs d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = tokenizer.texts_to_matrix(training_doc, mode='binary')\n",
    "\n",
    "print('Xtrain contient ', xTrain.shape[0], ' exemples de ', xTrain.shape[1], ' valeurs')\n",
    "print('une valeur = fréquence d\\'apparition des mots dans le vocabulaire global.')\n",
    "print('Ainsi, premier exemple d\\'entrainement = \\n', xTrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des sorties\n",
    "Les premiers vecteurs représente des exemples positifs. On crée une sortie constituée d'autant de valeurs [1] suivie de valeurs [0]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ytrain = suite de (0 (classement pour eval positive),  1 (classements pour éval négative))\n",
    "yTrain = zeros(len(positive_lines_train)+len(negative_lines_train))\n",
    "yTrain[:len(positive_lines_train)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples de test\n",
    "On ré-itère pour les exemples de test; en réutilisant le même 'Tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Sw6xGMHkUP1"
   },
   "outputs": [],
   "source": [
    "# build the test doc by alternating positive lines and negative lines\n",
    "test_Doc = list()\n",
    "test_Doc.extend(positive_lines_test)\n",
    "test_Doc.extend(negative_lines_test)\n",
    "# ask to the tokenizer to give the bag of words : a set of (word, frequence of use),\n",
    "# the words are already kown by the tokenizer*\n",
    "xTest = tokenizer.texts_to_matrix(test_Doc, mode='binary')\n",
    "print('Xtest contient ', xTest.shape[0], ' exemples de ', xTest.shape[1], ' valeurs de fréquence.')\n",
    "\n",
    "#sortie attendues des exemples de test, ytest = suite de (0, 1)\n",
    "yTest = zeros(len(positive_lines_test)+len(negative_lines_test))\n",
    "yTest[:len(positive_lines_test)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCJRY1mTlnls"
   },
   "source": [
    "---\n",
    "## Modèle de réseau pour l'analyse des theses\n",
    "Le réseau contient en couche d'entrée autant de neurones que de tokens retenus. \n",
    "Chaque exemple est codé sous forme d'une suite de 0 ou 1, d'une dimension égale au vocabulaire retenu.\n",
    "\n",
    "xTrain est donc une matrice de n exemples x m colonnes. m étant la dimension des vecteurs d'entrée, c-a-d le nombre de mots finalement pris en comptes, et donc le nombre de neurones de la 1er couche du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uruPxsQil-gZ"
   },
   "outputs": [],
   "source": [
    "# nb de neurones en entrée (= nb de tokens retenus)\n",
    "## shape[0] est la 1ere dimension d'un tableau (ici le nb d'exemples)\n",
    "## shape[1] est la 1ere dimension d'un tableau (ici le nb de tokens)\n",
    "n_words = xTrain.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67M4JqX2lNeu"
   },
   "outputs": [],
   "source": [
    "# TODO: définir la structure du réseau\n",
    "model = Sequential()\n",
    "## A VOUS !!!!!\n",
    "...\n",
    "...\n",
    "# compile network \n",
    "model.compile(### A VOUS .....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlYk8aKemF9b"
   },
   "outputs": [],
   "source": [
    "#TODO tester differents nb de tests (epochs)\n",
    "history = model.fit(xTrain, yTrain, \n",
    "                    validation_data=(xTest, yTest),\n",
    "                    epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Dessiner l'évolution de l'erreur et de la pertinence\n",
    "\n",
    "Utilisez un tableau de bord (tensorboard) pour illustrer les évolutions des apprentissages et tests\n",
    "(cf. [SolutionDetectionAlertesTensorBoard](https://github.com/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/SolutionDetectionAlertesTensorBoard.ipynb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObKC8oW7mROA"
   },
   "source": [
    "---\n",
    "## Estimation automatique. \n",
    "Réalisez un code qui permet à partir d'une fichier contenant des titres de thèses de créer un ficher csv indiquant le titre, le classement proposé (IA, NONIA, ?), et la valeur du classement (entre 0 & 1).\n",
    "Un classement entre 0.4 & 0.6 sera douteux et indiqué par '?'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## TRAVAIL A RENDRE\n",
    "\n",
    "* le choix du traitement des entrées (traitement des ' et -, stopwords, mots retenus, champs retenus (titre, directeur, auteur, ....))\n",
    "* Vous devrez tester différentes architectures de réseau, ainsi que différentes configuration (fonctions d'activation, méthodes de correction d'erreur, méthode de calcul de l'erreur, ...)\n",
    "* Vous enverrez/déposerez un fichier contenant pour chaque définition de réseau :\n",
    "    * l'architecture du réseau (un copier coller du bloc 'TODO: définir la structure du réseau')\n",
    "    * le déroulé de l'apprentissage (un copier coller du bloc résultat de 'TODO tester differents nb de tests (epochs)'\n",
    "    * les images du tableaux de bord (les diagrammes de l'onglet Scalars principalement)\n",
    "\n",
    "Au minimum 6 différents réseaux devront être fournis, en faisant varier : \n",
    " - le nb de couches,\n",
    " - les fonctions d'activation,\n",
    " - le mode de calcul de l'erreur commise\n",
    " - l'algorithme de correction (l ''optimizer'')\n",
    "  \n",
    "Vous testerez dans un premier temps sur des titres issus d'une ou deux feuilles du tableur, puis sur toutes les feuilles. Quel est l'impact sur l'apprentissage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de TP_DeepLearningM1TNSI.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
