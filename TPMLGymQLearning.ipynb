{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPMLGymQLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI](https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Présentation de Gym\n",
    "\n",
    "Voir la page d'introduction à [Gym](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outil AUTRE QUE COLAB (pyzo, jupyter lab, .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test de ML par Q-Learning pour atteindre l'objectif\n",
    "- Utiliser l'environnement `FrozenLake8x8-v1` (un labyrinthe en mode texte)\n",
    "- 4 actions sont possibles (Left(0), Down(1), Right(2), Up(3))\n",
    "  - l'adjectif \"Frozen\" signifie qu'une *action n'est pas déterministe !*\n",
    "    - à partir d'une case \"gelée\", aller à droite peut .. mener à droite, ou pas\n",
    "    - => intérêt du Q-Learning adapté à ce type d'environnement probabiliste\n",
    "- Le labyrinthe est ainsi composé de zones glacées, de puits, et d'un objectif\n",
    "\n",
    "En vous basant sur les codes présent dans l'article [Q-Learning en Java](http://emmanuel.adam.free.fr/site/spip.php?article134), programmer un algo de Q-Learning pour apprendre à atteindre l'objectif \n",
    "\n",
    "**N.B.** \n",
    "  - *Cet environnement fonctionne bien sous colab, jupyterlab.. quelques soucis de l'affichage de l'état courant (carré rouge) sous Pyzo....* \n",
    "  - Il est fortement conseillé de débuter avec un environnement déterministe pour évaluer la bonne marche de l'algo de Q-Learning que vous aurez développer..\n",
    "    - pour cela, lancer l'environnement avec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Etude de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='FrozenLake-v1', entry_point='gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4', 'is_slippery': True, 'render_mode': 'ansi'}, namespace=None, name='FrozenLake', version=1)\n",
      "Discrete(4)\n",
      "Discrete(16)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "S = Start, G = Goal, H = Hole, F = Frozen place\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode='ansi') \n",
    "# tester map_name=\"8x8\" pour l'environnement plus large\n",
    "print(env.spec)\n",
    "print(env.action_space) #ici 4 actions discrétisée\n",
    "print(env.observation_space) # ici 4x4 cellules possibles\n",
    "\n",
    "env.reset()\n",
    "print(env.render())\n",
    "print(\"S = Start, G = Goal, H = Hole, F = Frozen place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Test des actions\n",
    "\n",
    "En mode non déterministe, chaque action a 1 chance sur 3 de réussir..<br>\n",
    "*Exemple aller à droite mène 1/3 à droite, 1/3 en haut, 1/3 en bas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 4 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "###### Test des fonctions\n",
    "env.reset()\n",
    "action = 0\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 4 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 1\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 2\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 3\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note : \n",
    "## observation = position où se trouve l'agent (no case(i,j) = i*largeur+j)\n",
    "## reward = recompense\n",
    "## done = but atteint\n",
    "## truncated = etat feuille (non utilise ici)\n",
    "## info = proba de reussite de l'action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est clairement ici dans un environnement non déterministe (une même action à partir d'un même état ne mène pas toujours au même résultat); c'est le contexte de prédilection de l'algo de Q-Learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Mise en place de l'environnement en mode déterministe</font>\n",
    "Important, pour valider l'apprentissage de votre algorithme avant de passer en mode non-déterministe, il vaut mieux le tester sur un environnement où chaque action à 100% de réussite. Ci-dessous un exemple sur le mini labyrinthe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='ansi') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 6 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "pos° actuelle: 14 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "pos° actuelle: 15 ,gain: 1.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "actions = [2,2,1,1,1,2]\n",
    "env.reset()\n",
    "for a in actions:\n",
    "    observation, reward, done,_, info = env.step(a)\n",
    "    print(env.render())\n",
    "    print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Aide Python : (plusieurs possibilités)***\n",
    "- Il n'est pas nécessaire de créer des classes Etat, ....\n",
    "- Il faut pouvoir stocker les récompenses de chaque actions à partir de chaque case\n",
    "  - a priori, la création d'une matrice 4x4 de 4 valeurs peut être utile.\n",
    "  Plus simplement, on utilisera un tableau de n cases (pour les n etats) contenant m valeurs d'actions \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# cree un tableau de n cases x m actions\n",
    "q_actions = np.zeros((env.observation_space.n, env.action_space.n), np.float32)\n",
    "print(q_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recherche du max et de sa position*\n",
    "\n",
    "Reprenons la derniere modélisation sous forme de tableau de 4 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01297509 0.90311764 0.36200217 0.53466891]\n",
      " [0.21753126 0.1486138  0.69545345 0.39602564]\n",
      " [0.18348307 0.49858799 0.84607384 0.66118449]\n",
      " [0.4971295  0.49150569 0.56847206 0.12887271]\n",
      " [0.52758408 0.53058904 0.94523054 0.47123764]\n",
      " [0.78793628 0.58430868 0.49208643 0.92930246]\n",
      " [0.69077113 0.66702524 0.22414255 0.23459581]\n",
      " [0.35539074 0.54387106 0.77523979 0.40162692]\n",
      " [0.05791587 0.02296204 0.68227821 0.54090457]\n",
      " [0.00854811 0.26819492 0.96686916 0.60920238]\n",
      " [0.86511296 0.34100465 0.19701795 0.90531074]\n",
      " [0.48291018 0.06886919 0.28706745 0.58680669]\n",
      " [0.32912292 0.95590134 0.19309512 0.93499283]\n",
      " [0.44193389 0.95742732 0.24923035 0.52057711]\n",
      " [0.00340703 0.69191096 0.76140266 0.98655288]\n",
      " [0.42682983 0.90372205 0.03912378 0.61866962]]\n"
     ]
    }
   ],
   "source": [
    "#Exemple de recherche..\n",
    "#1. ici pour l'exemple, on place des valeurs aléatoires pour les actions\n",
    "Q = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 2, max= 0.8460738379472967 , en position  2\n",
      "la meilleure action de l'etat 2 serait donc  2\n"
     ]
    }
   ],
   "source": [
    "#max de la case 2\n",
    "max_2 = np.max(Q[2, :])\n",
    "position_max_2 = np.argmax(Q[2,:])\n",
    "print(\"case 2, max=\",max_2, \", en position \", position_max_2)\n",
    "print(\"la meilleure action de l'etat 2 serait donc \", position_max_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  -----\n",
    "# Développer la solution en Q-Learning\n",
    "\n",
    "Pour rappel, l'algorithme de Q-Learning est le suivant : \n",
    "\n",
    "- Initialiser la table des poids des arcs :\n",
    "  - $\\forall s \\forall a Q(s,a) \\gets 0$ \n",
    "- Pour n boucles d'apprentissage\n",
    "  - $\\lambda \\gets 1 ; \\epsilon \\gets 1 ;$\n",
    "  - $etat Courant \\gets  etat initial$\n",
    "  - Pour n' itérations ( n' = nombre max supposé d'actions pour atteindre l'objectif)\n",
    "    - $s \\gets  etat Courant$\n",
    "    - $nb \\gets random(0,1)$\n",
    "    - Si $nb < \\epsilon$ Alors\n",
    "      - Choisir aléatoirement une action a\n",
    "    - Sinon\n",
    "      - Choisir action la plus intéressante : $a \\gets argMax_{a'}(Q(s,a'))$\n",
    "    - Fin Si\n",
    "    - Effectuer action a :\n",
    "    - $s' \\gets  nouvel Etat Courant$\n",
    "    - calculer $Q(s,a) \\gets \\lambda \\times (r + \\gamma \\times max_{a'}(Q(s', a')) + (1-\\lambda ) \\times Q(s,a)$\n",
    "    - {{decrémenter}} les coefficients d'apprentissage et d'exploration :\n",
    "      - $\\lambda \\gets 0.99 \\times \\lambda$\n",
    "      - $\\epsilon \\gets 0.99 \\times \\epsilon$\n",
    "    - Si but atteint, casser la boucle n' (uniquement celle là)\n",
    "  - Fin pour n' itérations\n",
    "- Fin pour n boucles d'apprentissage  \n",
    "\n",
    "En reprenant les définitions précédentes, $Q(s,a)$ est équivalent à `Q[s][a]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation des variables\n",
    "\n",
    "On utilisera initialement les affectations suivantes pour les variables : \n",
    "- variables d'exploration, de récompense et d'apprentissage : \n",
    "  - $ \\lambda \\gets 0.8$\n",
    "  - $ \\gamma \\gets 0.998$\n",
    "- nombre de cycles : \n",
    "  - $ n \\gets 10000$ si grille 4x4, 50000 si grille 8x8\n",
    "  - $ n' \\gets 10$ si grille 4x4, 20 si grille 8x8\n",
    "    \n",
    "----    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Q-Learning simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##algorithme de Q-Learning simple\n",
    "def q_learn(nb_actions):\n",
    "    \"\"\"\n",
    "    effectue un cycle d'apprentissage/recherche de solution' via le Q-Learning simple\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : no de l'etape\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_r : recompense totale\n",
    "    r : recompense du dernier etat rencontre\n",
    "    states_list : liste des etats traverses\n",
    "    actions_list : liste des actions effectuees\n",
    "\n",
    "    \"\"\"\n",
    "    #A DEFINIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_qlearn(nb_episodes = 4000, nb_actions = 64):\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    global epsilon\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    solutions_list = []\n",
    "    epsilon = 1\n",
    "    for i in range(nb_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        total_r, r, states_list, actions_list = q_learn(nb_actions)\n",
    "        epsilon = epsilon * 0.999\n",
    "        # memorize if a solution has been found\n",
    "        if r == 1: solutions_list.append((i, total_r, states_list, actions_list))\n",
    "        \n",
    "    if(len(solutions_list) == 0): print(\"aucune solution trouvee !!\")\n",
    "\n",
    "    return solutions_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Affichage de du résultat\n",
    "Affichons maintenant la liste des actions via l'environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendu(solutions_list):\n",
    "    \"\"\" affiche la plus courte sequence d'actions permettant d'atteindre l'objectif q partir des solutions fournies\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions_list : liste des solutions trouvees\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    mini_sol = solutions_list[0]\n",
    "    for s in  solutions_list:\n",
    "        if len(s[2]) < len(mini_sol[2]): mini_sol = s\n",
    "    print(\"une solution en \", len(mini_sol[2]), \" etapes : \")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i in range(0, len(mini_sol[2])):\n",
    "        env.env.s = mini_sol[2][i]\n",
    "        print(\"action \", actions[mini_sol[3][i]])\n",
    "        print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST\n",
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn(3000, 50)\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "#relancer le bloc si pas de solution trouvee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Etudions l'historique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_frequence_sol(solutions_list):\n",
    "    \"\"\"\n",
    "    dessine la frequence de solution trouvees\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions : liste des solutions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    xs = [x[0] for x in solutions_list]\n",
    "    ys = [y[1] for y in solutions_list]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xs, ys, '.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## L'algorithme de Double Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Explication du double Q-Learning\n",
    "L'algo de **double** Q-Learning simple repose sur DEUX matrices nb_etats x nb_actions; la mise à jour des valeurs d'une action dans une grille utilise la valeur de la meilleure action suivante dans l'autre matrice.\n",
    "\n",
    "On réalise, plus ou moins alternativement :\n",
    "\n",
    "$a^* \\gets  argmax_{a} QA(s', a)$,\n",
    "\n",
    "$QA(s,a) \\gets QA(s,a) + \\lambda \\times (r + \\gamma \\times QB(s', a^*))-  QA(s,a))$ \n",
    "\n",
    "ou\n",
    "\n",
    "$b^* \\gets  argmax_{a} QB(s', a)$\n",
    "\n",
    "$QB(s,a) \\gets QB(s,a) + \\lambda \\times (r + \\gamma \\times QA(s', b^*))-  QB(s,a))$ \n",
    "\n",
    "avec\n",
    "  - $\\lambda$ : coef. d'apprentissage\n",
    "  - $\\gamma$ : coef. de réduction \n",
    "  - $r$ : récompense\n",
    "  \n",
    "Cette équation donne la qualité de l'action *a* à partir de l'état *s*.\n",
    "\n",
    "Initialement, à chaque état, une action est choisie aléatoirement (car toutes \"valent\" 0); puis au fil des tests les actions les plus valuées sont choisies. \n",
    "\n",
    "*Vous pouvez voir en fin de cette page des aides sur les fonctionnalités Python utiles pour la manipulation de tableaux.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiser la Q-Table\n",
    "# autant de cases que l'environnement en possède, \n",
    "# contenant autant de valeurs que d'actions possibles\n",
    "# donc ici une matrice 64 x 4\n",
    "QA = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "QB = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "lambda_learn = .3\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Travail à faire\n",
    "- Codez le double q learning dans une fonction proche de celle décrite pour le Q-Learning.\n",
    "- Reprenez la suite des fonctions développée pour lancer une résolution en mode déterministe et non déterministe. \n",
    "- Comparez les performances  (1ere solution trouvée, fréquence des solutions, ...)\n",
    "- reprenez au choix le simple QLearning, ou le Double Q-Learning et appliquez l'algorithme sur l'environnement CliffWalking-v0 (point de départ en x, arrivée en T, coût de -1 par action sur o, -100 par action sur C).\n",
    "   \n",
    "```o  o  o  o  o  o  o  o  o  o  o  o```\n",
    "\n",
    "```o  o  o  o  o  o  o  o  o  o  o  o```\n",
    "\n",
    "```o  o  o  o  o  o  o  o  o  o  o  o```\n",
    "\n",
    "```x  C  C  C  C  C  C  C  C  C  C  T```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
