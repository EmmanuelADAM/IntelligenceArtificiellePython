{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPMLGymQLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI](https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Présentation de Gym\n",
    "\n",
    "Voir la page d'introduction à [Gym](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de gym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outil AUTRE QUE COLAB (pyzo, jupyter lab, .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Utiliser l'environnement `FrozenLake en mode 8x8 en mode texte` (un labyrinthe)\n",
    "  - il est possible d'utiliser cet environnement en mode 'slipery', alors appliquer une *action n'est pas déterministe !*\n",
    "    - à partir d'une case \"gelée\", aller à droite peut .. mener à droite, ou en haut ou en bas....\n",
    "    - => intérêt du Q-Learning adapté à ce type d'environnement probabiliste\n",
    "- Le labyrinthe est ainsi composé de zones glacées, de puits, et d'un objectif contenant une récompense\n",
    "\n",
    "\n",
    "**N.B.** \n",
    "  - *Cet environnement fonctionne bien sous colab, jupyterlab.. quelques soucis de l'affichage de l'état courant (carré rouge) sous Pyzo....* \n",
    "  - Il est fortement conseillé de débuter avec un environnement déterministe pour évaluer la bonne marche de l'algo de Q-Learning que vous aurez développer.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Etude de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='FrozenLake-v1', entry_point='gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, disable_env_checker=False, kwargs={'map_name': '4x4', 'is_slippery': True, 'render_mode': 'ansi'}, namespace=None, name='FrozenLake', version=1, additional_wrappers=(), vector_entry_point=None)\n",
      "Discrete(4)\n",
      "Discrete(16)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "S = Start, G = Goal, H = Hole, F = Frozen place\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=True, render_mode='ansi') \n",
    "# tester map_name=\"8x8\" pour l'environnement plus large\n",
    "print(env.spec)\n",
    "print(env.action_space) #ici 4 actions discrétisée\n",
    "print(env.observation_space) # ici 4x4 cellules possibles\n",
    "\n",
    "env.reset()\n",
    "print(env.render())\n",
    "print(\"S = Start, G = Goal, H = Hole, F = Frozen place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Test des actions\n",
    "\n",
    "En mode non déterministe, chaque action a 1 chance sur 3 de réussir..<br>\n",
    "*Exemple aller à droite mène 1/3 à droite, 1/3 en haut, 1/3 en bas.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 4 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "###### Test des fonctions\n",
    "env.reset()\n",
    "action = 0\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 1\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 2\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 3\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "print(env.render())\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note : \n",
    "## observation = position où se trouve l'agent (no case(i,j) = i*largeur+j)\n",
    "## reward = recompense\n",
    "## done = but atteint\n",
    "## truncated = etat feuille (non utilise ici)\n",
    "## info = proba de reussite de l'action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est clairement ici dans un environnement non déterministe (une même action à partir d'un même état ne mène pas toujours au même résultat); c'est le contexte de prédilection de l'algo de Q-Learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Mise en place de l'environnement en mode déterministe</font>\n",
    "Important, pour valider l'apprentissage de votre algorithme avant de passer en mode non-déterministe, il vaut mieux le tester sur un environnement où chaque action à 100% de réussite. Ci-dessous un exemple sur le mini labyrinthe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='ansi') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 6 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "pos° actuelle: 14 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "pos° actuelle: 15 ,gain: 1.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "actions = [2,2,1,1,1,2]\n",
    "env.reset()\n",
    "for a in actions:\n",
    "    observation, reward, done,_, info = env.step(a)\n",
    "    print(env.render())\n",
    "    print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Aide Python : (plusieurs possibilités)***\n",
    "- Il n'est pas nécessaire de créer des classes Etat, ....\n",
    "- Il faut pouvoir stocker les récompenses de chaque actions à partir de chaque case\n",
    "  - a priori, la création d'une matrice 4x4 de 4 valeurs peut être utile.\n",
    "  Plus simplement, on utilisera un tableau de n cases (pour les n etats) contenant m valeurs d'actions \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# cree un tableau de n cases x m actions\n",
    "q_actions = np.zeros((env.observation_space.n, env.action_space.n), np.float32)\n",
    "print(q_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recherche du max et de sa position*\n",
    "\n",
    "Reprenons la derniere modélisation sous forme de tableau de 4 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69857432 0.57692712 0.48903335 0.80986133]\n",
      " [0.41487111 0.02495383 0.96939497 0.80744545]\n",
      " [0.28955053 0.8808106  0.49098469 0.12535302]\n",
      " [0.37804888 0.42886852 0.01846847 0.12935897]\n",
      " [0.76548417 0.50977692 0.69048414 0.41558513]\n",
      " [0.97712837 0.14180099 0.00344054 0.36719099]\n",
      " [0.57561994 0.91450199 0.93455483 0.47341385]\n",
      " [0.24095501 0.08432559 0.80339008 0.9577078 ]\n",
      " [0.69524998 0.90916935 0.75244151 0.68448496]\n",
      " [0.37155157 0.9473412  0.26786726 0.8404173 ]\n",
      " [0.87235816 0.96235686 0.96992848 0.68723202]\n",
      " [0.67714372 0.99042649 0.00211937 0.50907239]\n",
      " [0.69495648 0.2894569  0.16359583 0.42591248]\n",
      " [0.32998353 0.97697514 0.29909201 0.17314944]\n",
      " [0.55782669 0.66180988 0.07811949 0.29973459]\n",
      " [0.54647682 0.18838982 0.17677082 0.52170864]]\n"
     ]
    }
   ],
   "source": [
    "#Exemple de recherche..\n",
    "#1. ici pour l'exemple, on place des valeurs aléatoires pour les actions\n",
    "Q = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dans la case 2, la plus haute action a la valeur 0.91, elle est en position 1\n",
      "la meilleure action de l'etat 2 serait donc 1\n"
     ]
    }
   ],
   "source": [
    "#max de la case 2\n",
    "max_2 = np.max(Q[2, :])\n",
    "position_max_2 = np.argmax(Q[2,:])\n",
    "print(f\"dans la case 2, la plus haute action a la valeur {max_2:.2f}, elle est en position {position_max_2}\")\n",
    "print(f\"la meilleure action de l'etat 2 serait donc {position_max_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  -----\n",
    "# Développer la solution en Q-Learning\n",
    "\n",
    "Pour rappel, l'algorithme de Q-Learning est le suivant : \n",
    "\n",
    "- Initialiser la table des poids des arcs :\n",
    "  - $\\forall s \\forall a Q(s,a) \\gets 0$ \n",
    "- Pour n boucles d'apprentissage\n",
    "  - $\\lambda \\gets 1 ; \\epsilon \\gets 1 ;$\n",
    "  - $etat Courant \\gets  etat initial$\n",
    "  - Pour n' itérations ( n' = nombre max supposé d'actions pour atteindre l'objectif)\n",
    "    - $s \\gets  etat Courant$\n",
    "    - $nb \\gets random(0,1)$\n",
    "    - Si $nb < \\epsilon$ Alors\n",
    "      - Choisir aléatoirement une action a\n",
    "    - Sinon\n",
    "      - Choisir action la plus intéressante : $a \\gets argMax_{a'}(Q(s,a'))$\n",
    "    - Fin Si\n",
    "    - Effectuer action a :\n",
    "    - $s' \\gets  nouvel Etat Courant$\n",
    "    - calculer $Q(s,a) \\gets \\lambda \\times (r + \\gamma \\times max_{a'}(Q(s', a')) + (1-\\lambda ) \\times Q(s,a)$\n",
    "    - Si but atteint, casser la boucle n' (uniquement celle là)\n",
    "  - Fin pour n' itérations\n",
    "  - {{decrémenter}} le coefficient d'exploration :\n",
    "    - $\\epsilon \\gets 0.99 \\times \\epsilon$\n",
    "- Fin pour n boucles d'apprentissage  \n",
    "\n",
    "En reprenant les définitions précédentes, $Q(s,a)$ est équivalent à `Q[s][a]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation des variables\n",
    "\n",
    "On utilisera initialement les affectations suivantes pour les variables : \n",
    "- variables d'exploration, de récompense et d'apprentissage : \n",
    "  - $ \\lambda \\gets 0.8$\n",
    "  - $ \\gamma \\gets 0.998$\n",
    "- nombre de cycles : \n",
    "  - $ n \\gets 10000$ si grille 4x4, 50000 si grille 8x8\n",
    "  - $ n' \\gets 10$ si grille 4x4, 20 si grille 8x8\n",
    "    \n",
    "----    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_learn = .2\n",
    "gamma = 0.8\n",
    "epsilon = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Q-Learning simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "##algorithme de Q-Learning simple\n",
    "def q_learn(nb_actions=64):\n",
    "    \"\"\"\n",
    "    effectue un cycle d'apprentissage/recherche de solution' via le Q-Learning simple\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : no de l'etape\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_r : recompense totale\n",
    "    r : recompense du dernier etat rencontre\n",
    "    states_list : liste des etats traverses\n",
    "    actions_list : liste des actions effectuees\n",
    "\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    s = s[0]\n",
    "    total_r = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done and step < nb_actions:\n",
    "        step += 1\n",
    "        # Choose an action by greedily (with noise) picking from Q table\n",
    "        actions = Q[s, :]\n",
    "        if np.random.random()<epsilon or np.max(actions)==0:\n",
    "            a = np.random.randint(0, env.action_space.n-1)\n",
    "        else:\n",
    "            a = np.argmax(actions)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        new_state, r, done,_, _ = env.step(a)\n",
    "\n",
    "        # to privilegiate the shortest path, get negative reward every step\n",
    "        # if r == 0: r = -0.001\n",
    "\n",
    "        # Q-Learning\n",
    "        Q[s, a] = (1-lambda_learn)*Q[s, a] + lambda_learn*(r + gamma * np.max(Q[new_state, :]) - Q[s, a])\n",
    "        s = new_state\n",
    "        total_r = total_r + r\n",
    "        states_list.append(s)\n",
    "        actions_list.append(a)\n",
    "    return total_r, r, states_list, actions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_qlearn(nb_episodes = 4000, nb_actions = 64):\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    global epsilon\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    solutions_list = []\n",
    "    epsilon = 1\n",
    "    for i in range(nb_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        total_r, r, states_list, actions_list = q_learn(nb_actions)\n",
    "        epsilon = epsilon * 0.999\n",
    "        # memorize if a solution has been found\n",
    "        if r == 1: solutions_list.append((i, total_r, states_list, actions_list))\n",
    "        \n",
    "    if(len(solutions_list) == 0): print(\"aucune solution trouvee !!\")\n",
    "\n",
    "    return solutions_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Affichage de du résultat\n",
    "Affichons maintenant la liste des actions via l'environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendu(solutions_list):\n",
    "    \"\"\" affiche la plus courte sequence d'actions permettant d'atteindre l'objectif q partir des solutions fournies\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions_list : liste des solutions trouvees\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    mini_sol = solutions_list[0]\n",
    "    for s in  solutions_list:\n",
    "        if len(s[2]) < len(mini_sol[2]): mini_sol = s\n",
    "    print(\"une solution en \", len(mini_sol[2]), \" etapes : \")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i in range(0, len(mini_sol[2])):\n",
    "        env.step(mini_sol[3][i])#env.s = mini_sol[2][i]\n",
    "        print(\"action \", mini_sol[3][i])\n",
    "        print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une solution en  6  etapes : \n",
      "action  1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "action  1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "action  2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "action  1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "\n",
      "action  2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "action  2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##TEST\n",
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn(3000, 50)\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "#relancer le bloc si pas de solution trouvee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18,\n",
       "  1.0,\n",
       "  [1, 2, 3, 2, 1, 2, 6, 10, 14, 14, 15],\n",
       "  [2, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2]),\n",
       " (23, 1.0, [1, 2, 1, 2, 6, 10, 14, 15], [2, 2, 0, 2, 1, 1, 1, 2]),\n",
       " (24,\n",
       "  1.0,\n",
       "  [1, 0, 0, 1, 0, 0, 1, 0, 4, 4, 8, 8, 8, 9, 13, 14, 14, 13, 14, 15],\n",
       "  [2, 0, 0, 2, 0, 0, 2, 0, 1, 0, 1, 0, 0, 2, 1, 2, 1, 0, 2, 2]),\n",
       " (43,\n",
       "  1.0,\n",
       "  [1, 2, 1, 0, 4, 8, 9, 10, 9, 10, 14, 13, 14, 13, 14, 15],\n",
       "  [2, 2, 0, 0, 1, 1, 2, 2, 0, 2, 1, 0, 2, 0, 2, 2]),\n",
       " (56, 1.0, [4, 8, 8, 9, 10, 14, 14, 15], [np.int64(1), 1, 0, 2, 2, 1, 1, 2]),\n",
       " (65,\n",
       "  1.0,\n",
       "  [1, 0, 0, 0, 4, 8, 9, 10, 14, 14, 15],\n",
       "  [2, 0, 0, 0, 1, 1, 2, 2, 1, 1, 2]),\n",
       " (80, 1.0, [4, 8, 9, 13, 14, 13, 14, 15], [1, 1, 2, 1, 2, 0, 2, 2]),\n",
       " (81, 1.0, [1, 2, 6, 10, 14, 13, 14, 15], [2, 2, 1, 1, 1, 0, 2, 2]),\n",
       " (86,\n",
       "  1.0,\n",
       "  [4, 8, 8, 8, 9, 13, 13, 14, 13, 13, 13, 14, 14, 15],\n",
       "  [1, 1, 0, 0, 2, 1, 1, np.int64(2), 0, 1, 1, 2, 1, 2]),\n",
       " (128, 1.0, [4, 8, 8, 9, 13, 13, 13, 14, 15], [1, 1, 0, 2, 1, 1, 1, 2, 2]),\n",
       " (134, 1.0, [4, 8, 9, 10, 14, 15], [np.int64(1), 1, 2, 2, 1, 2]),\n",
       " (137, 1.0, [0, 4, 4, 8, 9, 10, 14, 15], [0, 1, 0, 1, 2, 2, 1, 2]),\n",
       " (159, 1.0, [0, 4, 8, 9, 10, 14, 15], [0, 1, 1, 2, 2, 1, 2]),\n",
       " (180, 1.0, [4, 8, 8, 9, 10, 14, 15], [np.int64(1), 1, 0, 2, 2, 1, 2]),\n",
       " (201,\n",
       "  1.0,\n",
       "  [4, 4, 4, 4, 8, 9, 10, 14, 13, 14, 14, 14, 15],\n",
       "  [1, 0, 0, 0, 1, 2, 2, np.int64(1), 0, 2, 1, 1, 2]),\n",
       " (245, 1.0, [0, 4, 8, 9, 10, 14, 15], [0, 1, 1, 2, 2, 1, 2]),\n",
       " (259,\n",
       "  1.0,\n",
       "  [4, 8, 9, 8, 9, 13, 13, 14, 15],\n",
       "  [1, 1, 2, 0, 2, 1, 1, np.int64(2), np.int64(2)]),\n",
       " (300,\n",
       "  1.0,\n",
       "  [4, 4, 4, 8, 8, 8, 8, 8, 9, 13, 14, 14, 14, 14, 15],\n",
       "  [np.int64(1), 0, 0, 1, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1, 2]),\n",
       " (354,\n",
       "  1.0,\n",
       "  [4, 8, 8, 9, 13, 14, 15],\n",
       "  [1, 1, 0, 2, 1, np.int64(2), np.int64(2)]),\n",
       " (370, 1.0, [1, 2, 1, 2, 6, 10, 14, 15], [2, 2, np.int64(0), 2, 1, 1, 1, 2]),\n",
       " (384,\n",
       "  1.0,\n",
       "  [4, 4, 8, 9, 10, 14, 13, 14, 15],\n",
       "  [1, 0, 1, 2, 2, np.int64(1), 0, np.int64(2), np.int64(2)]),\n",
       " (429,\n",
       "  1.0,\n",
       "  [4, 8, 9, 10, 14, 15],\n",
       "  [np.int64(1), 1, 2, 2, np.int64(1), np.int64(2)]),\n",
       " (453,\n",
       "  1.0,\n",
       "  [0, 0, 4, 4, 4, 4, 8, 9, 10, 14, 15],\n",
       "  [0, 0, np.int64(1), 0, 0, 0, 1, 2, 2, np.int64(1), np.int64(2)]),\n",
       " (569, 1.0, [0, 4, 8, 8, 9, 13, 14, 15], [0, 1, 1, 0, 2, 1, 2, np.int64(2)]),\n",
       " (593,\n",
       "  1.0,\n",
       "  [0, 4, 8, 9, 10, 14, 15],\n",
       "  [0, 1, 1, 2, 2, np.int64(1), np.int64(2)]),\n",
       " (647,\n",
       "  1.0,\n",
       "  [4, 8, 9, 13, 14, 13, 14, 13, 14, 14, 15],\n",
       "  [np.int64(1), 1, 2, 1, np.int64(2), 0, 2, 0, np.int64(2), 1, np.int64(2)]),\n",
       " (665,\n",
       "  1.0,\n",
       "  [4, 8, 9, 10, 14, 13, 14, 14, 15],\n",
       "  [np.int64(1), 1, 2, 2, 1, 0, np.int64(2), 1, np.int64(2)]),\n",
       " (689,\n",
       "  1.0,\n",
       "  [4, 8, 9, 10, 14, 14, 14, 15],\n",
       "  [np.int64(1), 1, 2, 2, np.int64(1), 1, 1, np.int64(2)]),\n",
       " (737,\n",
       "  1.0,\n",
       "  [1, 0, 4, 4, 4, 4, 8, 9, 13, 14, 15],\n",
       "  [2, 0, np.int64(1), 0, 0, 0, 1, 2, 1, np.int64(2), np.int64(2)]),\n",
       " (740,\n",
       "  1.0,\n",
       "  [0, 4, 8, 9, 13, 13, 13, 14, 15],\n",
       "  [0, np.int64(1), 1, 2, 1, 1, 1, np.int64(2), np.int64(2)])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Etudions l'historique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_frequence_sol(solutions_list):\n",
    "    \"\"\"\n",
    "    dessine la frequence de solution trouvees\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions : liste des solutions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    xs = [x[0] for x in solutions_list]\n",
    "    ys = [y[1] for y in solutions_list]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xs, ys, '.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Environnements personnalisés\n",
    "### Cases avec pénalités\n",
    "Utilisez la nouvelle classe `FrozenLakeWithPenalties` qui reprend FrozenLake et ajoute  des cases pénalisant le chemin (-1 pour chaque case 'P').\n",
    "Téléchargez la classe  [FrozenLakeWithPenalties](https://github.com/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/FrozenLakeWithPenalties.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FrozenLakeWithPenalties\n",
    "# Testez les terrains ci-dessous : \n",
    "# cartes avec penalites\n",
    "map_pen_4x4 = [ \"SFFF\", \"FFFF\",  \"FPPF\", \"HFFG\" ]\n",
    "map_pen_10x10 = [ \"SFFFPPPPFF\", \"FFFFPFFFFF\", \"FHFFPPPFFF\", \"FFFFPPPFFG\", \"FFFFPPPFFF\", \"HFFFFFFFFF\", \"FFFFFFFFFF\", \"FHHFFPFFFF\", \"FFPFFHFHFF\", \"HFFFFFPFFF\" ]\n",
    "\n",
    "env = FrozenLakeWithPenalties.FrozenLakeWithPenalties(desc=map_pen_10x10, is_slippery=True,   render_mode='ansi')\n",
    "#TODO: lancer la recherche de solutions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## L'algorithme de Double Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Explication du double Q-Learning\n",
    "L'algo de **double** Q-Learning simple repose sur DEUX matrices nb_etats x nb_actions; la mise à jour des valeurs d'une action dans une grille utilise la valeur de la meilleure action suivante dans l'autre matrice.\n",
    "\n",
    "On réalise, plus ou moins alternativement :\n",
    "\n",
    "$a^* \\gets  argmax_{a} QA(s', a)$,\n",
    "\n",
    "$QA(s,a) \\gets  \\lambda \\times (r + \\gamma \\times QB(s', a^*)) + (1-\\lambda)\\times QA(s,a) $\n",
    "\n",
    "ou\n",
    "\n",
    "$b^* \\gets  argmax_{a} QB(s', a)$\n",
    "\n",
    "$QB(s,a) \\gets  \\lambda \\times (r + \\gamma \\times QA(s', a^*)) + (1-\\lambda)\\times QB(s,a) $\n",
    "\n",
    "avec\n",
    "  - $\\lambda$ : coef. d'apprentissage\n",
    "  - $\\gamma$ : coef. de réduction \n",
    "  - $r$ : récompense\n",
    "  \n",
    "Cette équation donne la qualité de l'action *a* à partir de l'état *s*.\n",
    "\n",
    "Initialement, à chaque état, une action est choisie aléatoirement (car toutes \"valent\" 0); puis au fil des tests les actions les plus valuées sont choisies. \n",
    "\n",
    "*Vous pouvez voir en fin de cette page des aides sur les fonctionnalités Python utiles pour la manipulation de tableaux.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiser la Q-Table\n",
    "# autant de cases que l'environnement en possède, \n",
    "# contenant autant de valeurs que d'actions possibles\n",
    "# donc ici une matrice 64 x 4\n",
    "QA = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "QB = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "lambda_learn = .3\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Travail à faire\n",
    "- Codez le double q learning dans une fonction proche de celle décrite pour le Q-Learning.\n",
    "- Reprenez la suite des fonctions développée pour lancer une résolution en mode déterministe et non déterministe. \n",
    "- Comparez les performances  (1ere solution trouvée, fréquence des solutions, ...)\n",
    "- reprenez au choix le simple QLearning, ou le Double Q-Learning et appliquez l'algorithme sur l'environnement CliffWalking-v0 (point de départ en x, arrivée en T, coût de -1 par action sur o, -100 par action sur C).\n",
    "   \n",
    "```o  o  o  o  o  o  o  o  o  o  o  o```\n",
    "\n",
    "```o  o  o  o  o  o  o  o  o  o  o  o```\n",
    "\n",
    "```o  o  o  o  o  o  o  o  o  o  o  o```\n",
    "\n",
    "```x  C  C  C  C  C  C  C  C  C  C  T```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
