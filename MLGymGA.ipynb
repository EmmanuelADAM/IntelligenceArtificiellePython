{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/MLGymGA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI]((https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la page d'[introduction à Gym](https://github.com/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPMLIntroGym.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outil AUTRE QUE COLAB (pyzo, jupyter lab, .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>Sous colab</font>\n",
    "*Sous colab, l'affichage ne sera pas dynamique.. Il faudra passer soit par un rendu sous forme de plot (matplotlib), soit passer par une vidéo à enregistrer...*\n",
    "\n",
    "***Solution pour affichage statique***<br>\n",
    "*Il vous faut importer, seulement si vous utilisez colab, les packages suivants :*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install x11-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Charger l' environnement\n",
    "On utilise ici l'environnement `MountainCarContinuous-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Les valeurs des caractéristiques\n",
    "Pour `MountainCarContinuous-v0`, vous trouverez l'explication des valeurs [ici](https://github.com/openai/gym/wiki/MountainCarContinuous-v0).\n",
    "\n",
    "| No | Observation | Min | Max |\n",
    "|:--:|:--:|:--:|:--:|\n",
    "| 0 | Position | -1.2 | 0.6 |\n",
    "| 1 | Velocité | -0.07 | 0.07 |\n",
    "\n",
    "Vérification : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(MountainCarContinuous-v0)\n",
      "nombre et type d'actions =  Box(1,)\n",
      "valeurs d'actions les plus elevées =  [1.]\n",
      "valeurs d'actions les plus basses =  [-1.]\n",
      "nombre et type d'observations =  Box(2,)\n",
      "valeurs de  observations les plus basses =  [-1.2  -0.07]\n",
      "valeurs de observations les plus elevées =  [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "print(env.spec)\n",
    "print(\"nombre et type d'actions = \", env.action_space) \n",
    "print(\"valeurs d'actions les plus elevées = \", env.action_space.high) \n",
    "print(\"valeurs d'actions les plus basses = \", env.action_space.low)  \n",
    "print(\"nombre et type d'observations = \", env.observation_space) \n",
    "print(\"valeurs de  observations les plus basses = \", env.observation_space.low) \n",
    "print(\"valeurs de observations les plus elevées = \", env.observation_space.high) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Re)Initialiser l'environnement\n",
    "`env.reset()` initialise les variables d'environnement. L'exécution est obligatoire après avoir chargé l'environnement. Ici ce sont la position de départ et la vitesse.\n",
    "Pour avoir un environnement non déterministe, par défaut `env.reset()` retourne une position x aléatoire entre -0.4 et -0.6 et une vitesse initiale nulle : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40071066,  0.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais ici nous souhaitons partir du même point (-0.5) à vitesse nulle à chaque test de séquence d'actions. Avant chaque évaluation de séquence d'actions, ***on n'utilisera pas*** `env.reset()` par la suite et on écrira donc : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.state= np.array([-0.5, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Les actions\n",
    "- Dans `MountainCarContinuous-v0` : [-1.0] fait reculer le véhicule d'une force 1, [0] le met au point mort, [1.0] le fait avancer d'une force 1\n",
    "\n",
    "- ***Exécuter une action :***\n",
    "  - *exécution* : `observation, reward,done,info = env.step(action)`\n",
    "    - `observation`: ici la position du mobile et sa vitesse ([x, v])\n",
    "    - `reward`: utilité de l'état (récompense)\n",
    "    - `done` : booléen (True si but atteint)\n",
    "    - `info` : éventuelle information retournée par l'outil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= -0.49867684300416926\n",
      "v= 0.0013231569958307428\n",
      "x= -0.49604042641180685\n",
      "v= 0.002636416592362428\n"
     ]
    }
   ],
   "source": [
    "env.env.state= np.array([-0.5, 0])\n",
    "action = 1\n",
    "observation, reward,done,info = env.step([float(action)])\n",
    "print(\"x=\", observation[0])\n",
    "print(\"v=\", observation[1])\n",
    "observation, reward,done,info = env.step([float(action)])\n",
    "print(\"x=\", observation[0])\n",
    "print(\"v=\", observation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### La récompense (reward)\n",
    "Le *reward* dépend l'environnement d'exécution.. Dans `MountainCarContinuous-v0`, il est de `100(si le but est atteint)-le nombre d'actions au carré` !!\n",
    "Selon la méthode IA choisie, il peut être plus intéressant de définir sa popre fonction récompense \n",
    "(par exemple, on peut favoriser les séquences qui mène le plus loin possible..)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Test d'algorithme génétique\n",
    "## Test de programme évolutionnaire pour atteindre l'objectif\n",
    "- Utiliser l'environnement `MountainCarContinuous-v0`, car il est plus rapide \n",
    "- Programmez (en python) un algo génétique avec initialement les éléments suivants : \n",
    "  - sequence : tableau de valeurs entières -1, 0 ou 1.\n",
    "    - *(on pourrait prendre des valeurs réelles mais ici on se contentera d'entiers pour commencer)*\n",
    "  - 1 point de croisement (au centre)\n",
    "    - *(pour commencer, on pourrait proposer 2 points de croisement (au 1/3, et 2/3))*\n",
    "  - taille sequence : taille_seq = 80\n",
    "  - taille population initiale : taille_pop = 500\n",
    "  - taux de gènes mutants dans une séquence : taux_mut_seq = 0.1\n",
    "  - taux de séquences mutantes dans la population : taux_mut_pop = 0.2\n",
    "  - nombre max de cycles de reproductions : nb_cycles = 3000\n",
    "\n",
    "\n",
    "- Pour tester une séquence, il suffit de balayer le tableau et de lancer l'action correspondante..(en la transformant en un tableau de réel (1 -> [1.0])\n",
    "- Les 4 meilleures séquences (11,22,33,44) se reproduisent entre elles, donnant donc 12 fils : 12, 13, 14, 21, 23, 24, 31, 32, 34, 41, 42, 43.\n",
    "- Idéalement, un filtre doit permettre de supprimer les séquences identiques qui pourraient venir des croisements et des mutations.\n",
    "\n",
    "- Tester votre algo sur le nb de générations et afficher le rendu toutes les 100 générations et bien sûr le rendu de la meilleure séquence finale.\n",
    "  - (uniquement la position finale *colab*)\n",
    "  - (sous forme d'animation sous autre outil)\n",
    "\n",
    "\n",
    "- ***NB:*** \n",
    "  - Au plus simple il est possible d'utiliser une population (liste) de couples [sequence, utilite] où l'utilité serait la distance en x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-1, -1,  0]), 0]\n",
      "arrive en x= -0.51 , fini= False\n",
      "couple [actions, arrivee] =  [array([-1, -1,  0]), -0.5084984048091237]\n"
     ]
    }
   ],
   "source": [
    "#exemple :  \n",
    "env.env.state= np.array([-0.5, 0])\n",
    "tab = np.random.randint(-1, 2, 3) \n",
    "couple = [tab, 0]\n",
    "print(couple)\n",
    "for a in couple[0]:\n",
    "    obs, reward, done, info = env.step([a])\n",
    "print(\"arrive en x=\", round(obs[0],2), \", fini=\", done)\n",
    "couple[1] = obs[0]\n",
    "print(\"couple [actions, arrivee] = \", couple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus : \n",
    "- Adapter la récompense pour favoriser les séquences qui atteignent le but en un minimum d'actions utiles (0 étant considéré comme une action inutile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
